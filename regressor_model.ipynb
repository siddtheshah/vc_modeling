{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regressor_model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMwYqrG7UAfh6vQ//zmZTl5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddtheshah/vc_modeling/blob/master/regressor_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV182SZ1d2Es",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lswTEjiPegUo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e17f8948-ea12-489a-db4e-2dc69f4e26f8"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.sparse\n",
        "!pip install cityhash\n",
        "import cityhash\n",
        "import sklearn.decomposition\n",
        "\n",
        "print(pd.__version__)\n",
        "\n",
        "from copy import deepcopy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cityhash in /usr/local/lib/python3.6/dist-packages (0.2.3.post9)\n",
            "1.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2WrUtBZenFT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4eb845b0-2b1c-4f03-a0b0-292260dd4494"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJAKI4bLd5ZE",
        "colab_type": "text"
      },
      "source": [
        "# Read/Join Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0F3qbf0gF-K",
        "colab_type": "text"
      },
      "source": [
        "## Sparse Features\n",
        "These need to go through dimensionality reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mez-3_o6t-F-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_folder = '/content/gdrive/My Drive/vc_modeling/feature_extraction'\n",
        "\n",
        "sparse_category_features_array = scipy.sparse.load_npz(feature_folder + \"/category_features/category_features_large.npz\")\n",
        "sparse_region_features_array = scipy.sparse.load_npz(feature_folder + \"/region_features/region_features.npz\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv7LelH2eqbw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "af03ffc7-7657-48a4-fbb7-79ed34eb7461"
      },
      "source": [
        "# print(sparse_category_features_array)\n",
        "## Other features here!! Remember to sparsify the dataframes if they're dense!\n",
        "\n",
        "# print(sparse_category_features_array)\n",
        "\n",
        "category_features_array = scipy.sparse.coo_matrix(sparse_category_features_array, dtype=np.uint64)\n",
        "region_features_array = scipy.sparse.coo_matrix(sparse_region_features_array, dtype=np.uint64)\n",
        "\n",
        "print(category_features_array.getnnz())\n",
        "print(region_features_array.getnnz())\n",
        "\n",
        "category_features_df = pd.DataFrame.sparse.from_spmatrix(category_features_array)\n",
        "region_features_df = pd.DataFrame.sparse.from_spmatrix(region_features_array)\n",
        "\n",
        "print(np.shape(category_features_df))\n",
        "print(np.shape(region_features_df))\n",
        "\n",
        "print(\"{}\".format(category_features_df.iloc[0][0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2355537\n",
            "1681588\n",
            "(963967, 676)\n",
            "(842699, 1032)\n",
            "13685534557686295101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHqwaNc89w2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "bba60f92-3d13-4d15-b41a-a247487374b6"
      },
      "source": [
        "region_uuid = region_features_df.iloc[:, 0]\n",
        "category_uuid = category_features_df.iloc[:, 0]\n",
        "print(region_uuid)\n",
        "\n",
        "print(np.count_nonzero(category_uuid.isin(region_uuid)))\n",
        "check_value = 7551169957279540846\n",
        "# check_value = cityhash.CityHash64('ffffabce-6d4a-b3d1-13c0-4e90cedf5270')\n",
        "print(check_value)\n",
        "print(np.size(region_uuid[region_uuid == check_value]))\n",
        "print(np.size(category_uuid[category_uuid == check_value]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0         13685534557686295101\n",
            "1           764015621929367586\n",
            "2         10846552445983457719\n",
            "3          5087506707876194815\n",
            "4          9094535307341385563\n",
            "                  ...         \n",
            "842694     4626564123199390189\n",
            "842695     2978566027619600648\n",
            "842696     1747954284855665241\n",
            "842697     9744251496066876000\n",
            "842698     7648380604111671063\n",
            "Name: 0, Length: 842699, dtype: Sparse[uint64, 0]\n",
            "808944\n",
            "7551169957279540846\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X7VTKkb_Fd-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "b6c8a816-1e66-4500-9204-45b1cd92e013"
      },
      "source": [
        "join_base = category_features_df.set_index(0)\n",
        "join1 = region_features_df.set_index(0)\n",
        "sparse_join = join_base.join(join1, lsuffix='category_features', rsuffix='region_features')\n",
        "sparse_join = sparse_join.dropna()\n",
        "# features = category_features_df\n",
        "print(sparse_join)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                      1category_features  2category_features  ...  1030  1031\n",
            "0                                                             ...            \n",
            "13685534557686295101                 0.0                 0.0  ...   0.0   0.0\n",
            "764015621929367586                   1.0                 1.0  ...   0.0   0.0\n",
            "10846552445983457719                 0.0                 0.0  ...   0.0   0.0\n",
            "5087506707876194815                  0.0                 0.0  ...   0.0   0.0\n",
            "9094535307341385563                  0.0                 0.0  ...   0.0   0.0\n",
            "...                                  ...                 ...  ...   ...   ...\n",
            "16892862651199510638                 0.0                 0.0  ...   0.0   0.0\n",
            "7229716827056183679                  0.0                 0.0  ...   0.0   0.0\n",
            "4626564123199390189                  0.0                 0.0  ...   0.0   0.0\n",
            "2978566027619600648                  0.0                 0.0  ...   0.0   0.0\n",
            "9744251496066876000                  0.0                 0.0  ...   0.0   0.0\n",
            "\n",
            "[808944 rows x 1706 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCBshhYpnHfC",
        "colab_type": "text"
      },
      "source": [
        "## Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaScgb-7nKii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "outputId": "32e0dd02-c394-40cb-b2e6-9abe45e9cb29"
      },
      "source": [
        "svd = sklearn.decomposition.TruncatedSVD(n_components=100, n_iter=10)\n",
        "# Can't fit more than 10k samples or SVD will crash.\n",
        "# If the samples are well distributed, this might be OK.\n",
        "svd.fit(sparse_join[:15000])\n",
        "\n",
        "print(svd.explained_variance_ratio_)\n",
        "print(svd.explained_variance_ratio_.sum())\n",
        "print(svd.singular_values_)\n",
        "\n",
        "reduced_features = svd.transform(sparse_join)\n",
        "\n",
        "# lda = sklearn.decomposition.LatentDirichletAllocation(n_components=100,random_state=0, learning_method='online', total_samples=2e5)\n",
        "# lda.partial_fit(sparse_join)\n",
        "# reduced_features = lda.transform(sparse_join)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.06201002 0.04136426 0.03052181 0.02412492 0.02133094 0.01809638\n",
            " 0.01821206 0.01757454 0.01589228 0.01511112 0.01381074 0.01349057\n",
            " 0.01274772 0.01224814 0.01171326 0.01124586 0.01098865 0.01016308\n",
            " 0.00945056 0.00909412 0.00893294 0.00866497 0.0080689  0.0079854\n",
            " 0.00778647 0.00739946 0.00723822 0.00673296 0.00643826 0.00631925\n",
            " 0.0061844  0.00595845 0.00580656 0.00570427 0.00569912 0.00543318\n",
            " 0.00519543 0.00515399 0.00512568 0.00492261 0.00489803 0.00476558\n",
            " 0.00470362 0.00468775 0.00454391 0.00444036 0.0044279  0.00432054\n",
            " 0.00428805 0.00414271 0.0040916  0.00406363 0.0039766  0.00383421\n",
            " 0.00373742 0.00367794 0.00361784 0.0034927  0.00344725 0.00342721\n",
            " 0.00338314 0.00330981 0.00321606 0.00313725 0.00310952 0.00311294\n",
            " 0.00307007 0.00304925 0.00302563 0.00296626 0.00293493 0.00289766\n",
            " 0.00285058 0.00282081 0.00278331 0.00274471 0.00269561 0.00268947\n",
            " 0.00265179 0.00263574 0.00262881 0.00259017 0.00256825 0.00254525\n",
            " 0.00252571 0.00249433 0.00248445 0.00246696 0.00246281 0.00242219\n",
            " 0.00238579 0.00236791 0.00235489 0.00234876 0.00233212 0.00231905\n",
            " 0.00229358 0.00226369 0.00224926 0.00222378]\n",
            "0.6999386618261956\n",
            "[64.16250756 43.17911456 38.0775464  33.94071856 30.74950123 29.07410976\n",
            " 28.41298112 27.89555468 26.50816667 26.18558828 24.79913998 24.47350799\n",
            " 23.74176005 23.47177998 22.83546782 22.39979198 22.06906212 21.24918371\n",
            " 20.5210636  20.05456753 19.90621537 19.61166224 18.91779525 18.79241278\n",
            " 18.58220048 18.09775072 17.89219061 17.32573001 16.89803819 16.73148396\n",
            " 16.53827458 16.34472047 16.02407126 15.92792375 15.87536259 15.51732789\n",
            " 15.1782956  15.12371151 15.07827259 14.79408174 14.72176397 14.52553893\n",
            " 14.42233715 14.40454989 14.18119327 14.07707393 14.00462246 13.83081369\n",
            " 13.77036962 13.56245454 13.45731003 13.40434022 13.26934243 13.02060652\n",
            " 12.87024039 12.75382188 12.65697682 12.44740451 12.35903668 12.31007628\n",
            " 12.23070078 12.10743071 11.9256695  11.781603   11.74368269 11.7321528\n",
            " 11.65125039 11.61736898 11.57120621 11.45475008 11.41444129 11.32203981\n",
            " 11.23583922 11.18239    11.09355047 11.01858973 10.92924362 10.91101012\n",
            " 10.83148315 10.80278565 10.78205042 10.70183729 10.66195389 10.61001805\n",
            " 10.57676527 10.50989448 10.48205517 10.44997839 10.44218739 10.34883379\n",
            " 10.2709604  10.24297028 10.2040288  10.19184955 10.15525828 10.12615934\n",
            " 10.07145852 10.00478629  9.97996483  9.91639518]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7UxQ_SSdOZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "5b08aad0-5526-4021-8e6a-255d3fe21b8c"
      },
      "source": [
        "print(reduced_features)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.93442333e-02  1.01881367e-01  2.75295137e-01 ... -5.01619430e-04\n",
            "   3.16123014e-03 -3.39205537e-03]\n",
            " [ 1.22762868e+00 -5.75667147e-02  4.53293139e-02 ... -2.48072988e-01\n",
            "   2.74366063e-01  1.51284765e-01]\n",
            " [ 8.52595440e-02  1.09923636e-01  2.99163741e-01 ...  2.18662279e-02\n",
            "   3.88661477e-04 -7.44998317e-03]\n",
            " ...\n",
            " [ 4.96430409e-04  1.41098458e-03  2.81446000e-03 ... -5.01053818e-03\n",
            "  -8.24090503e-03 -1.08722485e-02]\n",
            " [ 9.65193781e-01 -2.14868786e-01 -1.12641455e-01 ...  1.01631988e-01\n",
            "  -1.98974917e-02  6.11482808e-02]\n",
            " [ 7.08646222e-03  2.48330429e-03  7.73408886e-03 ...  6.30110464e-02\n",
            "   1.69103963e-01 -1.27973087e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Glorc3xgOwy",
        "colab_type": "text"
      },
      "source": [
        "## Dense features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9T0sIOWgrsd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "17cdb349-3b22-4cd6-d579-a52537d5f0cf"
      },
      "source": [
        "founder_features = pd.read_csv('/content/gdrive/My Drive/vc_modeling/feature_extraction/founder_features/organization_founders_features.csv')\n",
        "founder_features['hash'] = founder_features['org_uuid'].apply(cityhash.CityHash64)\n",
        "founder_features = founder_features.set_index(['hash'], drop=True).drop(['org_uuid'], axis=1)\n",
        "\n",
        "founder_features_only = founder_features.dropna()\n",
        "\n",
        "print(founder_features.index)\n",
        "print(np.count_nonzero(founder_features.index.isin(sparse_join.index)))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UInt64Index([ 2705467411384211821, 13360469805707984821,  5744847760615345245,\n",
            "             13990853631299335829, 13073125021883633741, 17482404514494389050,\n",
            "              5766560289832673038,  8860273864704446424, 13093060529635406942,\n",
            "              3545623260843038609,\n",
            "             ...\n",
            "             15362439443771027468,  7042924928552646182,  4495539880758712748,\n",
            "               800095033165514664, 17142127829573402143, 11045203277162125921,\n",
            "             17622773241843276753,  9745467974249593237, 11854798340435595808,\n",
            "             12723450708549610702],\n",
            "            dtype='uint64', name='hash', length=198449)\n",
            "165327\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfhmbxNHn9hX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "8b207319-d404-4fda-b22d-78ce90894ede"
      },
      "source": [
        "sparse_data = pd.DataFrame(data=reduced_features, index=sparse_join.index, columns=range(np.shape(reduced_features)[1]))\n",
        "print(sparse_data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                            0         1   ...        98        99\n",
            "0                                         ...                    \n",
            "13685534557686295101  0.019344  0.101881  ...  0.003161 -0.003392\n",
            "764015621929367586    1.227629 -0.057567  ...  0.274366  0.151285\n",
            "10846552445983457719  0.085260  0.109924  ...  0.000389 -0.007450\n",
            "5087506707876194815   1.041919 -0.202393  ... -0.000872 -0.016956\n",
            "9094535307341385563   1.041080 -0.160387  ...  0.060246 -0.040377\n",
            "...                        ...       ...  ...       ...       ...\n",
            "16892862651199510638  0.013808  0.030725  ... -0.148291 -0.149886\n",
            "7229716827056183679   0.006609  0.029821  ... -0.006852 -0.002768\n",
            "4626564123199390189   0.000496  0.001411  ... -0.008241 -0.010872\n",
            "2978566027619600648   0.965194 -0.214869  ... -0.019897  0.061148\n",
            "9744251496066876000   0.007086  0.002483  ...  0.169104 -0.127973\n",
            "\n",
            "[808944 rows x 100 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBWRh2O7jHIZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "3c781bea-cf92-44bc-e623-7face452c2b9"
      },
      "source": [
        "all_features = sparse_data.join(founder_features, lsuffix='sparse', rsuffix='dense').fillna(0)\n",
        "all_features.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>founders_top_rank</th>\n",
              "      <th>founders_top_college</th>\n",
              "      <th>founders_max_degree_type_ordinal</th>\n",
              "      <th>founders_max_degree_count</th>\n",
              "      <th>founders_max_founded_other_org</th>\n",
              "      <th>founders_count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13685534557686295101</th>\n",
              "      <td>0.019344</td>\n",
              "      <td>0.101881</td>\n",
              "      <td>0.275295</td>\n",
              "      <td>0.770317</td>\n",
              "      <td>-0.392017</td>\n",
              "      <td>-0.308171</td>\n",
              "      <td>-0.074005</td>\n",
              "      <td>-0.098280</td>\n",
              "      <td>-0.030070</td>\n",
              "      <td>-0.144071</td>\n",
              "      <td>-0.008071</td>\n",
              "      <td>0.014282</td>\n",
              "      <td>-0.037295</td>\n",
              "      <td>-0.020789</td>\n",
              "      <td>0.036092</td>\n",
              "      <td>-0.071545</td>\n",
              "      <td>-0.086070</td>\n",
              "      <td>-0.031744</td>\n",
              "      <td>0.064637</td>\n",
              "      <td>0.036955</td>\n",
              "      <td>-0.036448</td>\n",
              "      <td>0.007317</td>\n",
              "      <td>0.003496</td>\n",
              "      <td>-0.015807</td>\n",
              "      <td>0.031912</td>\n",
              "      <td>-0.018393</td>\n",
              "      <td>-0.005094</td>\n",
              "      <td>-0.011109</td>\n",
              "      <td>-0.011276</td>\n",
              "      <td>-0.008199</td>\n",
              "      <td>0.023240</td>\n",
              "      <td>0.009102</td>\n",
              "      <td>-0.018469</td>\n",
              "      <td>-0.016408</td>\n",
              "      <td>-0.002777</td>\n",
              "      <td>0.001470</td>\n",
              "      <td>-0.022306</td>\n",
              "      <td>0.001711</td>\n",
              "      <td>-0.005529</td>\n",
              "      <td>-0.001948</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.006978</td>\n",
              "      <td>0.005297</td>\n",
              "      <td>0.001040</td>\n",
              "      <td>0.003455</td>\n",
              "      <td>-0.007144</td>\n",
              "      <td>-0.006633</td>\n",
              "      <td>0.003135</td>\n",
              "      <td>0.006671</td>\n",
              "      <td>-0.010396</td>\n",
              "      <td>0.003409</td>\n",
              "      <td>0.001292</td>\n",
              "      <td>0.001660</td>\n",
              "      <td>0.004507</td>\n",
              "      <td>-0.006544</td>\n",
              "      <td>-0.008661</td>\n",
              "      <td>-0.007365</td>\n",
              "      <td>-0.006274</td>\n",
              "      <td>0.006022</td>\n",
              "      <td>0.001709</td>\n",
              "      <td>-0.006762</td>\n",
              "      <td>-0.003619</td>\n",
              "      <td>-0.002418</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>-0.002625</td>\n",
              "      <td>0.012168</td>\n",
              "      <td>0.002082</td>\n",
              "      <td>-0.013992</td>\n",
              "      <td>-0.003680</td>\n",
              "      <td>-0.003141</td>\n",
              "      <td>0.003246</td>\n",
              "      <td>-0.000502</td>\n",
              "      <td>0.003161</td>\n",
              "      <td>-0.003392</td>\n",
              "      <td>20738.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764015621929367586</th>\n",
              "      <td>1.227629</td>\n",
              "      <td>-0.057567</td>\n",
              "      <td>0.045329</td>\n",
              "      <td>0.307684</td>\n",
              "      <td>-0.378097</td>\n",
              "      <td>1.342761</td>\n",
              "      <td>-0.475410</td>\n",
              "      <td>-0.082669</td>\n",
              "      <td>0.789706</td>\n",
              "      <td>0.067754</td>\n",
              "      <td>-0.074613</td>\n",
              "      <td>-0.046426</td>\n",
              "      <td>0.686327</td>\n",
              "      <td>-0.644056</td>\n",
              "      <td>0.845123</td>\n",
              "      <td>0.151952</td>\n",
              "      <td>0.006688</td>\n",
              "      <td>0.120635</td>\n",
              "      <td>-0.166380</td>\n",
              "      <td>0.068958</td>\n",
              "      <td>-0.078170</td>\n",
              "      <td>-0.011791</td>\n",
              "      <td>0.130785</td>\n",
              "      <td>0.080263</td>\n",
              "      <td>-0.067059</td>\n",
              "      <td>0.004475</td>\n",
              "      <td>0.004838</td>\n",
              "      <td>0.048233</td>\n",
              "      <td>-0.042638</td>\n",
              "      <td>-0.096127</td>\n",
              "      <td>-0.144643</td>\n",
              "      <td>0.276434</td>\n",
              "      <td>0.397256</td>\n",
              "      <td>-0.492176</td>\n",
              "      <td>0.957969</td>\n",
              "      <td>0.143199</td>\n",
              "      <td>0.104937</td>\n",
              "      <td>-0.237832</td>\n",
              "      <td>0.468004</td>\n",
              "      <td>0.037818</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.059530</td>\n",
              "      <td>0.022560</td>\n",
              "      <td>-0.194546</td>\n",
              "      <td>0.032373</td>\n",
              "      <td>-0.116263</td>\n",
              "      <td>0.070135</td>\n",
              "      <td>0.014322</td>\n",
              "      <td>0.046180</td>\n",
              "      <td>0.012347</td>\n",
              "      <td>-0.025068</td>\n",
              "      <td>0.061757</td>\n",
              "      <td>-0.040025</td>\n",
              "      <td>0.059608</td>\n",
              "      <td>0.012901</td>\n",
              "      <td>0.008808</td>\n",
              "      <td>-0.038358</td>\n",
              "      <td>0.013106</td>\n",
              "      <td>0.152483</td>\n",
              "      <td>-0.025216</td>\n",
              "      <td>0.019414</td>\n",
              "      <td>0.047884</td>\n",
              "      <td>0.009160</td>\n",
              "      <td>0.033771</td>\n",
              "      <td>0.078514</td>\n",
              "      <td>-0.043113</td>\n",
              "      <td>0.067004</td>\n",
              "      <td>0.090099</td>\n",
              "      <td>-0.073363</td>\n",
              "      <td>-0.079905</td>\n",
              "      <td>0.080013</td>\n",
              "      <td>-0.159157</td>\n",
              "      <td>-0.248073</td>\n",
              "      <td>0.274366</td>\n",
              "      <td>0.151285</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10846552445983457719</th>\n",
              "      <td>0.085260</td>\n",
              "      <td>0.109924</td>\n",
              "      <td>0.299164</td>\n",
              "      <td>0.861290</td>\n",
              "      <td>-0.430507</td>\n",
              "      <td>-0.285675</td>\n",
              "      <td>-0.125108</td>\n",
              "      <td>-0.085212</td>\n",
              "      <td>-0.052841</td>\n",
              "      <td>0.301601</td>\n",
              "      <td>0.113324</td>\n",
              "      <td>0.230529</td>\n",
              "      <td>0.696884</td>\n",
              "      <td>-0.310273</td>\n",
              "      <td>0.093155</td>\n",
              "      <td>-0.124344</td>\n",
              "      <td>-0.026946</td>\n",
              "      <td>0.032483</td>\n",
              "      <td>-0.001762</td>\n",
              "      <td>0.030034</td>\n",
              "      <td>-0.029834</td>\n",
              "      <td>-0.043316</td>\n",
              "      <td>-0.029376</td>\n",
              "      <td>-0.015402</td>\n",
              "      <td>-0.018304</td>\n",
              "      <td>-0.009062</td>\n",
              "      <td>-0.062101</td>\n",
              "      <td>-0.009256</td>\n",
              "      <td>-0.007697</td>\n",
              "      <td>-0.003740</td>\n",
              "      <td>-0.002182</td>\n",
              "      <td>-0.015565</td>\n",
              "      <td>-0.001157</td>\n",
              "      <td>-0.017887</td>\n",
              "      <td>0.003070</td>\n",
              "      <td>-0.019592</td>\n",
              "      <td>-0.045027</td>\n",
              "      <td>-0.029897</td>\n",
              "      <td>-0.008037</td>\n",
              "      <td>-0.001275</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014675</td>\n",
              "      <td>0.021212</td>\n",
              "      <td>-0.023287</td>\n",
              "      <td>0.009237</td>\n",
              "      <td>-0.034729</td>\n",
              "      <td>0.012161</td>\n",
              "      <td>0.006328</td>\n",
              "      <td>0.043441</td>\n",
              "      <td>-0.043361</td>\n",
              "      <td>0.012470</td>\n",
              "      <td>0.005556</td>\n",
              "      <td>-0.056102</td>\n",
              "      <td>-0.009241</td>\n",
              "      <td>-0.003833</td>\n",
              "      <td>0.006879</td>\n",
              "      <td>-0.025621</td>\n",
              "      <td>-0.031069</td>\n",
              "      <td>0.031094</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>-0.018766</td>\n",
              "      <td>0.024116</td>\n",
              "      <td>0.004497</td>\n",
              "      <td>0.005693</td>\n",
              "      <td>0.005347</td>\n",
              "      <td>-0.012256</td>\n",
              "      <td>0.013282</td>\n",
              "      <td>0.009630</td>\n",
              "      <td>-0.034428</td>\n",
              "      <td>-0.021929</td>\n",
              "      <td>0.017642</td>\n",
              "      <td>0.008498</td>\n",
              "      <td>0.021866</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>-0.007450</td>\n",
              "      <td>1735.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5087506707876194815</th>\n",
              "      <td>1.041919</td>\n",
              "      <td>-0.202393</td>\n",
              "      <td>-0.075857</td>\n",
              "      <td>0.109147</td>\n",
              "      <td>0.023643</td>\n",
              "      <td>-0.070532</td>\n",
              "      <td>-0.040474</td>\n",
              "      <td>-0.010898</td>\n",
              "      <td>-0.029603</td>\n",
              "      <td>0.387249</td>\n",
              "      <td>0.120527</td>\n",
              "      <td>0.245035</td>\n",
              "      <td>0.727058</td>\n",
              "      <td>-0.284426</td>\n",
              "      <td>0.063152</td>\n",
              "      <td>-0.060805</td>\n",
              "      <td>0.008412</td>\n",
              "      <td>0.042996</td>\n",
              "      <td>-0.034869</td>\n",
              "      <td>0.028239</td>\n",
              "      <td>-0.026684</td>\n",
              "      <td>-0.047513</td>\n",
              "      <td>-0.027730</td>\n",
              "      <td>-0.024480</td>\n",
              "      <td>-0.071931</td>\n",
              "      <td>-0.020039</td>\n",
              "      <td>-0.105923</td>\n",
              "      <td>-0.000254</td>\n",
              "      <td>0.024882</td>\n",
              "      <td>0.021468</td>\n",
              "      <td>-0.005955</td>\n",
              "      <td>-0.043904</td>\n",
              "      <td>0.056714</td>\n",
              "      <td>-0.095702</td>\n",
              "      <td>-0.036477</td>\n",
              "      <td>0.125357</td>\n",
              "      <td>0.432349</td>\n",
              "      <td>0.663626</td>\n",
              "      <td>-0.037359</td>\n",
              "      <td>-0.277946</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.079156</td>\n",
              "      <td>0.096766</td>\n",
              "      <td>0.041312</td>\n",
              "      <td>0.014758</td>\n",
              "      <td>-0.064243</td>\n",
              "      <td>-0.013374</td>\n",
              "      <td>0.008866</td>\n",
              "      <td>0.047162</td>\n",
              "      <td>-0.043204</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.010583</td>\n",
              "      <td>-0.058543</td>\n",
              "      <td>-0.015238</td>\n",
              "      <td>-0.007289</td>\n",
              "      <td>0.016533</td>\n",
              "      <td>-0.015691</td>\n",
              "      <td>-0.039492</td>\n",
              "      <td>0.020611</td>\n",
              "      <td>0.016583</td>\n",
              "      <td>-0.001244</td>\n",
              "      <td>0.011114</td>\n",
              "      <td>0.003738</td>\n",
              "      <td>0.010277</td>\n",
              "      <td>0.010243</td>\n",
              "      <td>-0.018302</td>\n",
              "      <td>0.008427</td>\n",
              "      <td>0.024730</td>\n",
              "      <td>-0.044477</td>\n",
              "      <td>-0.030381</td>\n",
              "      <td>0.032736</td>\n",
              "      <td>0.001875</td>\n",
              "      <td>0.029077</td>\n",
              "      <td>-0.000872</td>\n",
              "      <td>-0.016956</td>\n",
              "      <td>59.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9094535307341385563</th>\n",
              "      <td>1.041080</td>\n",
              "      <td>-0.160387</td>\n",
              "      <td>-0.084188</td>\n",
              "      <td>0.061716</td>\n",
              "      <td>-0.073297</td>\n",
              "      <td>0.128333</td>\n",
              "      <td>-0.087326</td>\n",
              "      <td>-0.078266</td>\n",
              "      <td>0.009992</td>\n",
              "      <td>0.070557</td>\n",
              "      <td>0.931831</td>\n",
              "      <td>-0.155165</td>\n",
              "      <td>-0.294538</td>\n",
              "      <td>-0.065990</td>\n",
              "      <td>-0.291644</td>\n",
              "      <td>-0.196866</td>\n",
              "      <td>0.069036</td>\n",
              "      <td>-0.085623</td>\n",
              "      <td>0.043731</td>\n",
              "      <td>-0.161204</td>\n",
              "      <td>0.021061</td>\n",
              "      <td>-0.309758</td>\n",
              "      <td>0.874170</td>\n",
              "      <td>0.387983</td>\n",
              "      <td>-0.360148</td>\n",
              "      <td>-0.023082</td>\n",
              "      <td>-0.078195</td>\n",
              "      <td>-0.031849</td>\n",
              "      <td>0.035605</td>\n",
              "      <td>0.024938</td>\n",
              "      <td>-0.125703</td>\n",
              "      <td>-0.039902</td>\n",
              "      <td>-0.044831</td>\n",
              "      <td>0.076490</td>\n",
              "      <td>0.027983</td>\n",
              "      <td>-0.290261</td>\n",
              "      <td>-0.022902</td>\n",
              "      <td>0.109369</td>\n",
              "      <td>-0.068197</td>\n",
              "      <td>-0.214508</td>\n",
              "      <td>...</td>\n",
              "      <td>0.036684</td>\n",
              "      <td>-0.000730</td>\n",
              "      <td>-0.014760</td>\n",
              "      <td>0.047040</td>\n",
              "      <td>0.012864</td>\n",
              "      <td>0.008369</td>\n",
              "      <td>-0.006737</td>\n",
              "      <td>0.009794</td>\n",
              "      <td>-0.004752</td>\n",
              "      <td>-0.047088</td>\n",
              "      <td>-0.014931</td>\n",
              "      <td>0.003079</td>\n",
              "      <td>-0.039029</td>\n",
              "      <td>-0.059524</td>\n",
              "      <td>0.028211</td>\n",
              "      <td>-0.031836</td>\n",
              "      <td>0.014193</td>\n",
              "      <td>0.012767</td>\n",
              "      <td>-0.034438</td>\n",
              "      <td>0.010045</td>\n",
              "      <td>-0.015692</td>\n",
              "      <td>-0.008552</td>\n",
              "      <td>0.016699</td>\n",
              "      <td>-0.006221</td>\n",
              "      <td>0.030590</td>\n",
              "      <td>-0.028374</td>\n",
              "      <td>-0.022767</td>\n",
              "      <td>-0.006314</td>\n",
              "      <td>-0.021006</td>\n",
              "      <td>0.007919</td>\n",
              "      <td>0.019242</td>\n",
              "      <td>-0.085243</td>\n",
              "      <td>0.060246</td>\n",
              "      <td>-0.040377</td>\n",
              "      <td>38172.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 106 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                             0  ...  founders_count\n",
              "0                               ...                \n",
              "13685534557686295101  0.019344  ...             2.0\n",
              "764015621929367586    1.227629  ...             1.0\n",
              "10846552445983457719  0.085260  ...             1.0\n",
              "5087506707876194815   1.041919  ...             9.0\n",
              "9094535307341385563   1.041080  ...             2.0\n",
              "\n",
              "[5 rows x 106 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ_HWAMlSWim",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "9736ddb1-7e7b-48f3-eb96-1e37b7e313eb"
      },
      "source": [
        "import sklearn.preprocessing\n",
        "\n",
        "features_array = sklearn.preprocessing.normalize(all_features, norm='max', axis=0, copy=False)\n",
        "features = pd.DataFrame(data=features_array, index=all_features.index, columns=range(np.shape(all_features)[1]))\n",
        "print(features)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                           0         1    ...       104       105\n",
            "0                                         ...                    \n",
            "13685534557686295101  0.011911  0.043331  ...  0.012821  0.060606\n",
            "764015621929367586    0.755909 -0.024484  ...  0.012821  0.030303\n",
            "10846552445983457719  0.052498  0.046752  ...  0.000000  0.030303\n",
            "5087506707876194815   0.641559 -0.086080  ...  0.025641  0.272727\n",
            "9094535307341385563   0.641042 -0.068214  ...  0.012821  0.060606\n",
            "...                        ...       ...  ...       ...       ...\n",
            "16892862651199510638  0.008502  0.013068  ...  0.000000  0.000000\n",
            "7229716827056183679   0.004069  0.012683  ...  0.000000  0.060606\n",
            "4626564123199390189   0.000306  0.000600  ...  0.000000  0.000000\n",
            "2978566027619600648   0.594315 -0.091386  ...  0.000000  0.000000\n",
            "9744251496066876000   0.004363  0.001056  ...  0.000000  0.000000\n",
            "\n",
            "[808944 rows x 106 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6InV-GK0e49l",
        "colab_type": "text"
      },
      "source": [
        "# Read Regression Targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzWperFWfA4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_folder = '/content/gdrive/My Drive/vc_modeling/regression_targets/'\n",
        "marks = [200, 500, 1000, 2000]\n",
        "\n",
        "regression_marks = {}\n",
        "for mark in marks:\n",
        "  regression_marks[mark] = pd.read_csv(target_folder + str(mark) + '.csv')[['hash', 'initial_valuation', 'log_valuation_factor']].set_index('hash')\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9hQhkQcY4Jr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "9cbff608-24bc-4ba7-fb8b-f7ad903bfb3b"
      },
      "source": [
        "mark_data = regression_marks[200] # pd.read_pickle(\"/content/gdrive/My Drive/vc_modeling/regression_targets/200.pkl\")\n",
        "# mark_data = mark_data[mark_data['log_valuation_factor'] > 0]\n",
        "print(mark_data)\n",
        "print(features)\n",
        "print(np.count_nonzero(mark_data.index.isin(features.index)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                      initial_valuation  log_valuation_factor\n",
            "hash                                                         \n",
            "2053339725337568679         413036820.0              0.000000\n",
            "13360469805707984821          3000000.0              0.266595\n",
            "12201126308526847683         47500000.0              0.000000\n",
            "17482404514494389050          2157880.0              0.000000\n",
            "16923506324318240851          7500000.0              0.000000\n",
            "...                                 ...                   ...\n",
            "4057326795460754576            552105.0              0.000000\n",
            "17393885764651115266         20000000.0              1.011831\n",
            "14785394360939257924        125000000.0              0.000000\n",
            "15207057269115911424         31150000.0              0.000000\n",
            "12723450708549610702            75000.0              1.688395\n",
            "\n",
            "[144569 rows x 2 columns]\n",
            "                           0         1    ...       104       105\n",
            "0                                         ...                    \n",
            "13685534557686295101  0.011911  0.043331  ...  0.012821  0.060606\n",
            "764015621929367586    0.755909 -0.024484  ...  0.012821  0.030303\n",
            "10846552445983457719  0.052498  0.046752  ...  0.000000  0.030303\n",
            "5087506707876194815   0.641559 -0.086080  ...  0.025641  0.272727\n",
            "9094535307341385563   0.641042 -0.068214  ...  0.012821  0.060606\n",
            "...                        ...       ...  ...       ...       ...\n",
            "16892862651199510638  0.008502  0.013068  ...  0.000000  0.000000\n",
            "7229716827056183679   0.004069  0.012683  ...  0.000000  0.060606\n",
            "4626564123199390189   0.000306  0.000600  ...  0.000000  0.000000\n",
            "2978566027619600648   0.594315 -0.091386  ...  0.000000  0.000000\n",
            "9744251496066876000   0.004363  0.001056  ...  0.000000  0.000000\n",
            "\n",
            "[808944 rows x 106 columns]\n",
            "138159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnfflhAPyChF",
        "colab_type": "text"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57NUdx-Xx-9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train Models\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection\n",
        "\n",
        "def regression_analysis(model, train_data, train_values, test_data, test_values):\n",
        "    predicted_train_values = model.predict(train_data)\n",
        "    predicted_test_values = model.predict(test_data)\n",
        "\n",
        "    print(\"Sample values: \", predicted_test_values[:5], test_values[:5])\n",
        "\n",
        "    train_mse = sklearn.metrics.mean_squared_error(train_values, predicted_train_values)\n",
        "    test_mse = sklearn.metrics.mean_squared_error(test_values, predicted_test_values)\n",
        "    train_explained_variance = sklearn.metrics.explained_variance_score(train_values, predicted_train_values)\n",
        "    test_explained_variance = sklearn.metrics.explained_variance_score(test_values, predicted_test_values)\n",
        "\n",
        "    print(\"Train MSE: \", train_mse)\n",
        "    print(\"Train Explained Variance Score: \", train_explained_variance)\n",
        "    print(\"Test MSE: \", test_mse)\n",
        "    print(\"Test Explained Variance Score: \", test_explained_variance)\n",
        "\n",
        "    return model\n",
        "\n",
        "def classification_analysis(model, train_data, train_values, test_data, test_values):\n",
        "    train_values_predicted = model.predict(train_data)\n",
        "    threshold = np.average(train_values_predicted)\n",
        "    train_prediction = train_values_predicted > threshold\n",
        "    train_prediction = train_prediction.astype(np.int32)\n",
        "\n",
        "    test_values_predicted = model.predict(test_data)\n",
        "    test_prediction = test_values_predicted > threshold\n",
        "    test_prediction = test_prediction.astype(np.int32)\n",
        "\n",
        "    train_labels = train_values > threshold\n",
        "    train_labels = train_labels.astype(np.int32)\n",
        "\n",
        "    test_labels = test_values > threshold\n",
        "    test_labels = test_labels.astype(np.int32)\n",
        "\n",
        "    confusion_matrix_large = pd.DataFrame(sklearn.metrics.confusion_matrix(test_labels, test_prediction, labels=[1, 0]),\n",
        "                                    columns=['positive', 'negative'], index=['Truth is +', 'Truth is -'])\n",
        "    print(\"Confusion:\\n\", confusion_matrix_large)\n",
        "    test_acc = sum(test_labels==test_prediction)/len(test_labels)\n",
        "    print(\"Test accuracy: \", test_acc)\n",
        "    train_acc = sum(train_labels==train_prediction)/len(train_labels)\n",
        "    print(\"Train accuracy: \", train_acc)\n",
        "\n",
        "\n",
        "    # # Use the metrics.roc_curve function to get the true positive rate (tpr) and false positive rate (fpr)\n",
        "    # fpr, tpr, thresholds = sklearn.metrics.roc_curve(test_labels, test_probabilities)\n",
        "\n",
        "    # # Get the area under the curve (AUC)\n",
        "    # auc = np.mean(cross_val_score(model, test_data, test_labels, scoring=\"roc_auc\", cv=5))\n",
        "    # print(\"AUC = \" , str(round(auc, 2)))\n",
        "\n",
        "    # # Plot the ROC curve\n",
        "\n",
        "    # plt.xlabel(\"False positive rate (fpr)\")\n",
        "    # plt.ylabel(\"True positive rate (tpr)\")\n",
        "    # plt.plot(fpr, tpr, label='model')\n",
        "    # plt.plot([0, 1], [0, 1], color='k', label=\"random\")\n",
        "    # plt.legend(loc='best')\n",
        "\n",
        "    # plt.figure()\n",
        "    # plt.xlabel(\"Recall\")\n",
        "    # plt.ylabel(\"Precision\")\n",
        "    # precision, recall, _ = sklearn.metrics.precision_recall_curve(test_labels, test_probabilities)\n",
        "    # plt.plot(recall, precision)\n",
        "\n",
        "def train_model_over_mark(model, input_data, mark, filter_unknown=True, hyperparams=None):\n",
        "  print(\"\\nRESULTS FOR\", str(mark), \"DAY MARK:\\n\")\n",
        "  # print(features)\n",
        "  mark_data = regression_marks[mark].dropna()\n",
        "  # print(mark_data)\n",
        "  if filter_unknown:\n",
        "    mark_data = mark_data[mark_data['log_valuation_factor'] != 0]\n",
        "  # Select the data that we have regression targets for\n",
        "\n",
        "  data = input_data[input_data.index.isin(mark_data.index)].sort_index()\n",
        "  # print(data)\n",
        "\n",
        "\n",
        "  # Select the column with log_valuation_factor.\n",
        "  values = mark_data[mark_data.index.isin(input_data.index)]['log_valuation_factor'].sort_index()\n",
        "  # print(values)\n",
        "\n",
        "  train_data, test_data, train_values, test_values = sklearn.model_selection.train_test_split(data, values, test_size=0.25)\n",
        "  if hyperparams:\n",
        "    print(\"Conducting Grid Search\")\n",
        "    search = sklearn.model_selection.GridSearchCV(model, hyperparams)\n",
        "    search.fit(train_data, train_values)\n",
        "    model = search.best_estimator_\n",
        "  \n",
        "  print(\"Trained on\", str(np.shape(train_data)[0]), \"rows.\")\n",
        "  model.fit(train_data, train_values)\n",
        "  regression_analysis(model, train_data, train_values, test_data, test_values)\n",
        "  classification_analysis(model, train_data, train_values, test_data, test_values)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qdukp1WNVzc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "03afa964-51f8-49fb-a39c-c061b1235b9a"
      },
      "source": [
        "print(founder_features_only[:5])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                      founders_top_rank  ...  founders_count\n",
            "hash                                     ...                \n",
            "2705467411384211821            265041.0  ...               4\n",
            "13360469805707984821             6551.0  ...               2\n",
            "13990853631299335829           147007.0  ...               1\n",
            "17482404514494389050           785716.0  ...               1\n",
            "15337030219814864514             1672.0  ...               1\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol1UlbPc-Njh",
        "colab_type": "text"
      },
      "source": [
        "## Lasso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4zrYwJg-M0E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "97f328ba-5d62-441d-ab6f-c18d5949d002"
      },
      "source": [
        "import sklearn.linear_model\n",
        "\n",
        "for mark in marks:\n",
        "  lasso_model = sklearn.linear_model.LassoCV()\n",
        "  train_model_over_mark(lasso_model, founder_features_only, mark, filter_unknown=True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "RESULTS FOR 200 DAY MARK:\n",
            "\n",
            "Trained on 15194 rows.\n",
            "Sample values:  [0.47289684 0.48227043 0.45136202 0.4787192  0.47818043] hash\n",
            "9622546503644133800     1.148421\n",
            "13564795469384548957   -0.013797\n",
            "8146852167827177215     0.268857\n",
            "14221072989662828273    0.248886\n",
            "9182469554284269992     0.334900\n",
            "Name: log_valuation_factor, dtype: float64\n",
            "Train MSE:  0.9234066704219968\n",
            "Train Explained Variance Score:  0.005736404158916963\n",
            "Test MSE:  0.9533087896836367\n",
            "Test Explained Variance Score:  0.007994415095862184\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +      1476       599\n",
            "Truth is -      1910      1080\n",
            "Test accuracy:  0.504639684106614\n",
            "Train accuracy:  0.5080294853231538\n",
            "\n",
            "RESULTS FOR 500 DAY MARK:\n",
            "\n",
            "Trained on 15258 rows.\n",
            "Sample values:  [0.92139    0.79760271 0.48490741 0.89010718 0.80555982] hash\n",
            "15952436086378540884    2.148845\n",
            "10361184551274432178    3.018954\n",
            "12377247905022160551    0.082780\n",
            "6064338030755353548     1.213023\n",
            "9567037658667064656     1.632980\n",
            "Name: log_valuation_factor, dtype: float64\n",
            "Train MSE:  1.6881328266887798\n",
            "Train Explained Variance Score:  0.01414953345463743\n",
            "Test MSE:  1.7187970884922166\n",
            "Test Explained Variance Score:  0.012412669867449044\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +      1700       638\n",
            "Truth is -      1712      1037\n",
            "Test accuracy:  0.5380381364261844\n",
            "Train accuracy:  0.5413553545680955\n",
            "\n",
            "RESULTS FOR 1000 DAY MARK:\n",
            "\n",
            "Trained on 15289 rows.\n",
            "Sample values:  [1.26732711 1.22545729 1.29389886 1.28648232 1.09590036] hash\n",
            "17361501222604697331    0.192372\n",
            "158242100734589447      2.379568\n",
            "17135747191865872501    1.648659\n",
            "16741708675925665778   -0.630707\n",
            "2529125259440496193    -0.144392\n",
            "Name: log_valuation_factor, dtype: float64\n",
            "Train MSE:  2.36122955460192\n",
            "Train Explained Variance Score:  0.02329832391960529\n",
            "Test MSE:  2.461841154323993\n",
            "Test Explained Variance Score:  0.02330362187910473\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +      1733       640\n",
            "Truth is -      1661      1063\n",
            "Test accuracy:  0.5485579752795762\n",
            "Train accuracy:  0.5545163189221008\n",
            "\n",
            "RESULTS FOR 2000 DAY MARK:\n",
            "\n",
            "Trained on 15271 rows.\n",
            "Sample values:  [1.21390355 1.07706961 1.23041718 1.31657732 1.38983797] hash\n",
            "9602025352273174844     1.979672\n",
            "8062899401687931551     0.978232\n",
            "17427097278148038474    3.269437\n",
            "5424625337261194441     1.611106\n",
            "4713509336504760078    -0.693147\n",
            "Name: log_valuation_factor, dtype: float64\n",
            "Train MSE:  3.0098997847500053\n",
            "Train Explained Variance Score:  0.03244682105928265\n",
            "Test MSE:  3.013807977960089\n",
            "Test Explained Variance Score:  0.02998971554009977\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +      1794       620\n",
            "Truth is -      1609      1068\n",
            "Test accuracy:  0.5621685327047732\n",
            "Train accuracy:  0.5600157160631262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ksr5r4cuc0",
        "colab_type": "text"
      },
      "source": [
        "## Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5PRMVseeLBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "2b7df573-3ce2-411d-d224-217528a9260c"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "models = []\n",
        "hyperdict = {'n_estimators': [20, 40, 60, 80, 100], 'min_samples_split':[10, 20, 30]}\n",
        "\n",
        "for mark in marks:\n",
        "  tree_model = RandomForestRegressor(criterion='mse', max_depth=4)\n",
        "  train_model_over_mark(tree_model, features, mark, filter_unknown=True)\n",
        "  models.append(tree_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "RESULTS FOR 200 DAY MARK:\n",
            "\n",
            "Trained on 39445 rows.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyz3a63EcWxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk-T0iO2xtaQ",
        "colab_type": "text"
      },
      "source": [
        "## SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_GVqKW6xxhl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "1eb2b90e-733a-4de3-cde2-dc9387d79f12"
      },
      "source": [
        "import sklearn.linear_model\n",
        "\n",
        "sgd = sklearn.linear_model.SGDRegressor()\n",
        "\n",
        "for mark in marks:\n",
        "  train_model_over_mark(sgd, features, mark, filter_unknown=True)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "RESULTS FOR 200 DAY MARK:\n",
            "\n",
            "Trained on 39445 rows.\n",
            "Sample values:  [0.22003474 0.33866962 0.3883994  0.30914981 0.27774328] hash\n",
            "13764208312783600756    0.299257\n",
            "7712191640504818461    -0.037245\n",
            "520587253883061091      0.870304\n",
            "7921057564085320398     1.447795\n",
            "9676512273645742147     0.297620\n",
            "Name: log_valuation_factor, dtype: float64\n",
            "Train MSE:  0.862958986492888\n",
            "Train Explained Variance Score:  0.015644671691800727\n",
            "Test MSE:  0.8371483697020786\n",
            "Test Explained Variance Score:  0.01302847815889685\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +      2769      2586\n",
            "Truth is -      3220      4574\n",
            "Test accuracy:  0.558445509164195\n",
            "Train accuracy:  0.5648117632146026\n",
            "\n",
            "RESULTS FOR 500 DAY MARK:\n",
            "\n",
            "Trained on 39574 rows.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-c50948a7fb68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmark\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmarks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mtrain_model_over_mark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-e146b0867627>\u001b[0m in \u001b[0;36mtrain_model_over_mark\u001b[0;34m(model, input_data, mark, filter_unknown, hyperparams)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trained on\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rows.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m   \u001b[0mregression_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0mclassification_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m   1218\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                          \u001b[0mintercept_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m                          sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         self._partial_fit(X, y, alpha, C, loss, learning_rate,\n\u001b[1;32m   1179\u001b[0m                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m                           intercept_init)\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m         if (self.tol is not None and self.tol > -np.inf\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m   1099\u001b[0m                      max_iter, sample_weight, coef_init, intercept_init):\n\u001b[1;32m   1100\u001b[0m         X, y = check_X_y(X, y, \"csr\", copy=False, order='C', dtype=np.float64,\n\u001b[0;32m-> 1101\u001b[0;31m                          accept_large_sparse=False)\n\u001b[0m\u001b[1;32m   1102\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmcPRYYmzrp-",
        "colab_type": "text"
      },
      "source": [
        "# SVR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE03MWnSztYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "62fd8e7c-88e7-4150-8a04-7b788bd3b0ed"
      },
      "source": [
        "import sklearn.svm\n",
        "\n",
        "for mark in marks:\n",
        "  svm = sklearn.svm.SVR(C=0.001)\n",
        "  train_model_over_mark(svm, features, mark, filter_unknown=True)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "RESULTS FOR 200 DAY MARK:\n",
            "\n",
            "Trained on 39445 rows.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-a6f245a88811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmark\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmarks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mtrain_model_over_mark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-e99fdf045a53>\u001b[0m in \u001b[0;36mtrain_model_over_mark\u001b[0;34m(model, input_data, mark, filter_unknown, hyperparams)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trained on\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rows.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m   \u001b[0mregression_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0mclassification_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLN28KUEnDVG",
        "colab_type": "text"
      },
      "source": [
        "## MultiLayer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53e0q3p3nHWP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d72d691-7af4-4454-a0a9-bb6c089a5ba3"
      },
      "source": [
        "import sklearn.neural_network\n",
        "\n",
        "models = []\n",
        "for mark in marks:\n",
        "  mlp = sklearn.neural_network.MLPRegressor(alpha=1e-2, hidden_layer_sizes=(100, 50, 20), max_iter=500)\n",
        "  train_model_over_mark(mlp, features, mark, filter_unknown=True)\n",
        "  models.append(mlp)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "RESULTS FOR 200 DAY MARK:\n",
            "\n",
            "Trained on 13498 rows.\n",
            "Sample values:  [ 1.01448316  0.13523465  2.98108355 -0.20288775  0.64604466] [ 1.41098697  8.26614023 -0.21043823  0.19912993  0.37140677]\n",
            "Train MSE:  0.42701258332359915\n",
            "Train Explained Variance Score:  0.5338952743648366\n",
            "Test MSE:  1.625637056147282\n",
            "Test Explained Variance Score:  -0.7780854593892346\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +       883       882\n",
            "Truth is -      1414      1321\n",
            "Test accuracy:  0.48977777777777776\n",
            "Train accuracy:  0.7315157801155727\n",
            "\n",
            "RESULTS FOR 500 DAY MARK:\n",
            "\n",
            "Trained on 10535 rows.\n",
            "Sample values:  [-1.34096084  0.61557298  0.33235761  3.82992509  0.38427185] [-2.67065643  1.17937934  3.40119738  0.81166578 -0.40932986]\n",
            "Train MSE:  0.6380924964172782\n",
            "Train Explained Variance Score:  0.638961100389262\n",
            "Test MSE:  3.5991605344502013\n",
            "Test Explained Variance Score:  -0.9857997549251816\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +       935       886\n",
            "Truth is -       878       813\n",
            "Test accuracy:  0.4977220956719818\n",
            "Train accuracy:  0.7849074513526341\n",
            "\n",
            "RESULTS FOR 1000 DAY MARK:\n",
            "\n",
            "Trained on 6560 rows.\n",
            "Sample values:  [ 1.05781406  1.30474663 -1.05851661  1.92204268  0.92198715] [0.0489254  3.11100483 2.24947527 1.51686585 1.25157881]\n",
            "Train MSE:  0.7724381055076045\n",
            "Train Explained Variance Score:  0.7003580166179628\n",
            "Test MSE:  5.053429087997823\n",
            "Test Explained Variance Score:  -1.1440242461352508\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +       485       470\n",
            "Truth is -       620       612\n",
            "Test accuracy:  0.5016003657978967\n",
            "Train accuracy:  0.8161585365853659\n",
            "\n",
            "RESULTS FOR 2000 DAY MARK:\n",
            "\n",
            "Trained on 2553 rows.\n",
            "Sample values:  [ 0.86963635 -0.22676619  4.31809311  1.59530808  1.90210519] [2.61006979 2.99573252 0.90628098 2.32238772 3.22360911]\n",
            "Train MSE:  0.8114841836092799\n",
            "Train Explained Variance Score:  0.7343021649337962\n",
            "Test MSE:  6.885263843361652\n",
            "Test Explained Variance Score:  -1.274811207858312\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +       189       214\n",
            "Truth is -       198       250\n",
            "Test accuracy:  0.5158636897767332\n",
            "Train accuracy:  0.8460634547591069\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzw2iKOr6FL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "57cc8b71-44a1-4449-c8f0-2f517fb45d62"
      },
      "source": [
        "# Save the models\n",
        "import pickle\n",
        "print(models)\n",
        "for mark, model in zip(marks, models):\n",
        "  print(model)\n",
        "  pickle.dump(model, open('/content/gdrive/My Drive/vc_modeling/models/' + str(mark) + '.pkl', 'wb'))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[MLPRegressor(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
            "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
            "             hidden_layer_sizes=(100, 50, 20), learning_rate='constant',\n",
            "             learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
            "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
            "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "             warm_start=False), MLPRegressor(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
            "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
            "             hidden_layer_sizes=(100, 50, 20), learning_rate='constant',\n",
            "             learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
            "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
            "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "             warm_start=False), MLPRegressor(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
            "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
            "             hidden_layer_sizes=(100, 50, 20), learning_rate='constant',\n",
            "             learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
            "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
            "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "             warm_start=False), MLPRegressor(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
            "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
            "             hidden_layer_sizes=(100, 50, 20), learning_rate='constant',\n",
            "             learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
            "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
            "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "             warm_start=False)]\n",
            "MLPRegressor(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
            "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
            "             hidden_layer_sizes=(100, 50, 20), learning_rate='constant',\n",
            "             learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
            "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
            "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "             warm_start=False)\n",
            "MLPRegressor(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
            "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
            "             hidden_layer_sizes=(100, 50, 20), learning_rate='constant',\n",
            "             learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
            "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
            "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "             warm_start=False)\n",
            "MLPRegressor(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
            "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
            "             hidden_layer_sizes=(100, 50, 20), learning_rate='constant',\n",
            "             learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
            "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
            "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "             warm_start=False)\n",
            "MLPRegressor(activation='relu', alpha=0.01, batch_size='auto', beta_1=0.9,\n",
            "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
            "             hidden_layer_sizes=(100, 50, 20), learning_rate='constant',\n",
            "             learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
            "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
            "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
            "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
            "             warm_start=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UivkBRqlA4UM",
        "colab_type": "text"
      },
      "source": [
        "# Export Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVg9x0AH6qoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the models\n",
        "import pickle\n",
        "models = []\n",
        "for mark in marks:\n",
        "\n",
        "  model = pickle.load(open('/content/gdrive/My Drive/vc_modeling/models/' + str(int(mark)) + '.pkl', 'rb'))\n",
        "  models.append(model)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK73IoyvvQPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "org_info = pd.read_csv(\"/content/gdrive/My Drive/vc_modeling/data/crunchbase_bulk_export/organizations.csv\")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65FZRRvezF6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "9482ee33-d11f-40a3-d5bc-65361511595d"
      },
      "source": [
        "org_info['hash'] = org_info['uuid'].apply(cityhash.CityHash64)\n",
        "org_info = org_info.set_index('hash')\n",
        "org_info = org_info[['uuid', 'name', 'created_at']]\n",
        "org_info.head()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uuid</th>\n",
              "      <th>name</th>\n",
              "      <th>created_at</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hash</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13685534557686295101</th>\n",
              "      <td>e1393508-30ea-8a36-3f96-dd3226033abd</td>\n",
              "      <td>Wetpaint</td>\n",
              "      <td>2007-05-25 13:51:27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764015621929367586</th>\n",
              "      <td>bf4d7b0e-b34d-2fd8-d292-6049c4f7efc7</td>\n",
              "      <td>Zoho</td>\n",
              "      <td>2007-05-26 02:30:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10846552445983457719</th>\n",
              "      <td>5f2b40b8-d1b3-d323-d81a-b7a8e89553d0</td>\n",
              "      <td>Digg</td>\n",
              "      <td>2007-05-26 03:03:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10693046220981818130</th>\n",
              "      <td>f4d5ab44-058b-298b-ea81-380e6e9a8eec</td>\n",
              "      <td>Omidyar Network</td>\n",
              "      <td>2007-05-26 03:21:34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5087506707876194815</th>\n",
              "      <td>df662812-7f97-0b43-9d3e-12f64f504fbb</td>\n",
              "      <td>Facebook</td>\n",
              "      <td>2007-05-26 04:22:15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      uuid  ...           created_at\n",
              "hash                                                        ...                     \n",
              "13685534557686295101  e1393508-30ea-8a36-3f96-dd3226033abd  ...  2007-05-25 13:51:27\n",
              "764015621929367586    bf4d7b0e-b34d-2fd8-d292-6049c4f7efc7  ...  2007-05-26 02:30:28\n",
              "10846552445983457719  5f2b40b8-d1b3-d323-d81a-b7a8e89553d0  ...  2007-05-26 03:03:23\n",
              "10693046220981818130  f4d5ab44-058b-298b-ea81-380e6e9a8eec  ...  2007-05-26 03:21:34\n",
              "5087506707876194815   df662812-7f97-0b43-9d3e-12f64f504fbb  ...  2007-05-26 04:22:15\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "881FEeGvxx-M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "4407bf1c-e48c-4047-ba48-c74d4331bcf3"
      },
      "source": [
        "import math\n",
        "\n",
        "org_info_join = org_info.copy()\n",
        "\n",
        "print(marks)\n",
        "for model, mark in zip(models, marks):\n",
        "  truth = regression_marks[mark][['initial_valuation', 'log_valuation_factor']].copy()\n",
        "  # print(truth)\n",
        "  prediction_array = model.predict(features)\n",
        "\n",
        "  prediction = pd.DataFrame(data=prediction_array, index=features.index, columns=['prediction_' + str(mark)])\n",
        "  uuid_hash = features.index\n",
        "  pred_truth = truth.join(prediction)\n",
        "  pred_truth = pred_truth.rename(columns={'log_valuation_factor':'truth_' + str(mark)})\n",
        "  org_info_join = org_info_join.join(pred_truth, rsuffix=mark)\n",
        "\n",
        "org_info_join org_info_join.set_index('uuid', drop=True)\n",
        "org_info_join.head()\n",
        "\n",
        "org_info_join.to_csv('/content/gdrive/My Drive/vc_modeling/model_output/predictions.csv')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[200, 500, 1000, 2000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uuid</th>\n",
              "      <th>name</th>\n",
              "      <th>created_at</th>\n",
              "      <th>initial_valuation</th>\n",
              "      <th>truth_200</th>\n",
              "      <th>prediction_200</th>\n",
              "      <th>initial_valuation500</th>\n",
              "      <th>truth_500</th>\n",
              "      <th>prediction_500</th>\n",
              "      <th>initial_valuation1000</th>\n",
              "      <th>truth_1000</th>\n",
              "      <th>prediction_1000</th>\n",
              "      <th>initial_valuation2000</th>\n",
              "      <th>truth_2000</th>\n",
              "      <th>prediction_2000</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hash</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13685534557686295101</th>\n",
              "      <td>e1393508-30ea-8a36-3f96-dd3226033abd</td>\n",
              "      <td>Wetpaint</td>\n",
              "      <td>2007-05-25 13:51:27</td>\n",
              "      <td>26250000.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051553</td>\n",
              "      <td>26250000.0</td>\n",
              "      <td>0.339276</td>\n",
              "      <td>1.352991</td>\n",
              "      <td>26250000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.194802</td>\n",
              "      <td>26250000.0</td>\n",
              "      <td>2.80565</td>\n",
              "      <td>2.923317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764015621929367586</th>\n",
              "      <td>bf4d7b0e-b34d-2fd8-d292-6049c4f7efc7</td>\n",
              "      <td>Zoho</td>\n",
              "      <td>2007-05-26 02:30:28</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10846552445983457719</th>\n",
              "      <td>5f2b40b8-d1b3-d323-d81a-b7a8e89553d0</td>\n",
              "      <td>Digg</td>\n",
              "      <td>2007-05-26 03:03:23</td>\n",
              "      <td>14000000.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.353350</td>\n",
              "      <td>14000000.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.669800</td>\n",
              "      <td>14000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.216546</td>\n",
              "      <td>14000000.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.861921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10693046220981818130</th>\n",
              "      <td>f4d5ab44-058b-298b-ea81-380e6e9a8eec</td>\n",
              "      <td>Omidyar Network</td>\n",
              "      <td>2007-05-26 03:21:34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5087506707876194815</th>\n",
              "      <td>df662812-7f97-0b43-9d3e-12f64f504fbb</td>\n",
              "      <td>Facebook</td>\n",
              "      <td>2007-05-26 04:22:15</td>\n",
              "      <td>2500000.0</td>\n",
              "      <td>2.416558</td>\n",
              "      <td>2.876117</td>\n",
              "      <td>2500000.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.548189</td>\n",
              "      <td>2500000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.083458</td>\n",
              "      <td>2500000.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>1.773893</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                      uuid  ... prediction_2000\n",
              "hash                                                        ...                \n",
              "13685534557686295101  e1393508-30ea-8a36-3f96-dd3226033abd  ...        2.923317\n",
              "764015621929367586    bf4d7b0e-b34d-2fd8-d292-6049c4f7efc7  ...             NaN\n",
              "10846552445983457719  5f2b40b8-d1b3-d323-d81a-b7a8e89553d0  ...        1.861921\n",
              "10693046220981818130  f4d5ab44-058b-298b-ea81-380e6e9a8eec  ...             NaN\n",
              "5087506707876194815   df662812-7f97-0b43-9d3e-12f64f504fbb  ...        1.773893\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R0LykdHtIRt",
        "colab_type": "text"
      },
      "source": [
        "# Comparison to Random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x9kg7OHtHpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}