{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regressor_model.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNAlEhkwXTwP2uymmkBhPXW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddtheshah/vc_modeling/blob/master/regressor_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV182SZ1d2Es",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lswTEjiPegUo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0b61b4df-5eb4-495e-bf23-ab6a70e794b1"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.sparse\n",
        "!pip install cityhash\n",
        "import cityhash\n",
        "import sklearn.decomposition\n",
        "\n",
        "print(pd.__version__)\n",
        "\n",
        "from copy import deepcopy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cityhash in /usr/local/lib/python3.6/dist-packages (0.2.3.post9)\n",
            "1.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2WrUtBZenFT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a3b5caeb-effa-4139-8f15-922a2997ea1b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJAKI4bLd5ZE",
        "colab_type": "text"
      },
      "source": [
        "# Read/Join Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0F3qbf0gF-K",
        "colab_type": "text"
      },
      "source": [
        "## Sparse Features\n",
        "These need to go through dimensionality reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mez-3_o6t-F-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_folder = '/content/gdrive/My Drive/vc_modeling/feature_extraction'\n",
        "\n",
        "sparse_category_features_array = scipy.sparse.load_npz(feature_folder + \"/category_features/category_features_large.npz\")\n",
        "sparse_region_features_array = scipy.sparse.load_npz(feature_folder + \"/region_features/region_features.npz\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv7LelH2eqbw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "43000288-4b0d-4c4c-83ea-43b2ccf37184"
      },
      "source": [
        "# print(sparse_category_features_array)\n",
        "## Other features here!! Remember to sparsify the dataframes if they're dense!\n",
        "\n",
        "# print(sparse_category_features_array)\n",
        "\n",
        "category_features_array = scipy.sparse.coo_matrix(sparse_category_features_array, dtype=np.uint64)\n",
        "region_features_array = scipy.sparse.coo_matrix(sparse_region_features_array, dtype=np.uint64)\n",
        "\n",
        "print(category_features_array.getnnz())\n",
        "print(region_features_array.getnnz())\n",
        "\n",
        "category_features_df = pd.DataFrame.sparse.from_spmatrix(category_features_array)\n",
        "region_features_df = pd.DataFrame.sparse.from_spmatrix(region_features_array)\n",
        "\n",
        "print(np.shape(category_features_df))\n",
        "print(np.shape(region_features_df))\n",
        "\n",
        "print(\"{}\".format(category_features_df.iloc[0][0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2355537\n",
            "1681588\n",
            "(963967, 676)\n",
            "(842699, 1032)\n",
            "13685534557686295101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHqwaNc89w2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "97b905ca-2a47-49da-bf4f-be112de0d2d8"
      },
      "source": [
        "region_uuid = region_features_df.iloc[:, 0]\n",
        "category_uuid = category_features_df.iloc[:, 0]\n",
        "print(region_uuid)\n",
        "\n",
        "print(np.count_nonzero(category_uuid.isin(region_uuid)))\n",
        "check_value = 7551169957279540846\n",
        "# check_value = cityhash.CityHash64('ffffabce-6d4a-b3d1-13c0-4e90cedf5270')\n",
        "print(check_value)\n",
        "print(np.size(region_uuid[region_uuid == check_value]))\n",
        "print(np.size(category_uuid[category_uuid == check_value]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0         13685534557686295101\n",
            "1           764015621929367586\n",
            "2         10846552445983457719\n",
            "3          5087506707876194815\n",
            "4          9094535307341385563\n",
            "                  ...         \n",
            "842694     4626564123199390189\n",
            "842695     2978566027619600648\n",
            "842696     1747954284855665241\n",
            "842697     9744251496066876000\n",
            "842698     7648380604111671063\n",
            "Name: 0, Length: 842699, dtype: Sparse[uint64, 0]\n",
            "808944\n",
            "7551169957279540846\n",
            "1\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-X7VTKkb_Fd-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "1407893c-af2c-421d-d0c4-24ab0b4fa3a3"
      },
      "source": [
        "join_base = category_features_df.set_index(0)\n",
        "join1 = region_features_df.set_index(0)\n",
        "sparse_join = join_base.join(join1, lsuffix='category_features', rsuffix='region_features')\n",
        "sparse_join = sparse_join.dropna()\n",
        "# features = category_features_df\n",
        "print(sparse_join)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                      1category_features  2category_features  ...  1030  1031\n",
            "0                                                             ...            \n",
            "13685534557686295101                 0.0                 0.0  ...   0.0   0.0\n",
            "764015621929367586                   1.0                 1.0  ...   0.0   0.0\n",
            "10846552445983457719                 0.0                 0.0  ...   0.0   0.0\n",
            "5087506707876194815                  0.0                 0.0  ...   0.0   0.0\n",
            "9094535307341385563                  0.0                 0.0  ...   0.0   0.0\n",
            "...                                  ...                 ...  ...   ...   ...\n",
            "16892862651199510638                 0.0                 0.0  ...   0.0   0.0\n",
            "7229716827056183679                  0.0                 0.0  ...   0.0   0.0\n",
            "4626564123199390189                  0.0                 0.0  ...   0.0   0.0\n",
            "2978566027619600648                  0.0                 0.0  ...   0.0   0.0\n",
            "9744251496066876000                  0.0                 0.0  ...   0.0   0.0\n",
            "\n",
            "[808944 rows x 1706 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCBshhYpnHfC",
        "colab_type": "text"
      },
      "source": [
        "## Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaScgb-7nKii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "outputId": "18bc48f0-cc76-44d0-9e20-b615dcfd486e"
      },
      "source": [
        "svd = sklearn.decomposition.TruncatedSVD(n_components=100, n_iter=10)\n",
        "# Can't fit more than 10k samples or SVD will crash.\n",
        "# If the samples are well distributed, this might be OK.\n",
        "svd.fit(sparse_join[:15000])\n",
        "\n",
        "print(svd.explained_variance_ratio_)\n",
        "print(svd.explained_variance_ratio_.sum())\n",
        "print(svd.singular_values_)\n",
        "\n",
        "reduced_features = svd.transform(sparse_join)\n",
        "\n",
        "# lda = sklearn.decomposition.LatentDirichletAllocation(n_components=100,random_state=0, learning_method='online', total_samples=2e5)\n",
        "# lda.partial_fit(sparse_join)\n",
        "# reduced_features = lda.transform(sparse_join)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.06201002 0.04136426 0.03052181 0.02412492 0.02133094 0.01809638\n",
            " 0.01821206 0.01757454 0.01589228 0.01511112 0.01381074 0.01349057\n",
            " 0.01274772 0.01224814 0.01171326 0.01124586 0.01098865 0.01016308\n",
            " 0.00945056 0.00909412 0.00893294 0.00866497 0.0080689  0.0079854\n",
            " 0.00778647 0.00739946 0.00723822 0.00673296 0.00643826 0.00631926\n",
            " 0.0061844  0.00595846 0.00580656 0.00570427 0.00569912 0.00543318\n",
            " 0.00519543 0.00515399 0.00512568 0.00492261 0.00489803 0.00476558\n",
            " 0.00470362 0.00468774 0.00454391 0.00444037 0.00442791 0.00432053\n",
            " 0.00428805 0.00414272 0.00409159 0.00406363 0.00397661 0.00383421\n",
            " 0.00373743 0.00367794 0.00361783 0.00349267 0.00344719 0.00342721\n",
            " 0.00338314 0.00330971 0.00321613 0.00313719 0.00310964 0.003113\n",
            " 0.00307043 0.00304931 0.0030259  0.00296645 0.00293466 0.00289825\n",
            " 0.00285019 0.00282051 0.00278403 0.00274516 0.00269483 0.00269146\n",
            " 0.00265022 0.00263723 0.00262737 0.00258918 0.00256648 0.00254527\n",
            " 0.00252416 0.00249816 0.00247924 0.00246801 0.00246343 0.00242158\n",
            " 0.00239408 0.00236862 0.0023634  0.00235042 0.00233281 0.00232238\n",
            " 0.00230212 0.00226358 0.00223751 0.00223019]\n",
            "0.6999617093931907\n",
            "[64.16250756 43.17911456 38.0775464  33.94071856 30.74950123 29.07410976\n",
            " 28.41298112 27.89555468 26.50816667 26.18558828 24.79913998 24.47350799\n",
            " 23.74176005 23.47177998 22.83546782 22.39979198 22.06906212 21.24918371\n",
            " 20.5210636  20.05456753 19.90621537 19.61166224 18.91779525 18.79241278\n",
            " 18.58220048 18.09775072 17.89219061 17.32573001 16.89803819 16.73148396\n",
            " 16.53827458 16.34472046 16.02407126 15.92792375 15.87536259 15.51732789\n",
            " 15.17829561 15.12371154 15.07827259 14.79408176 14.72176392 14.5255389\n",
            " 14.42233717 14.40454989 14.18119332 14.07707361 14.0046225  13.83081403\n",
            " 13.77037028 13.56245461 13.45730973 13.40434066 13.26934266 13.02060635\n",
            " 12.87023859 12.75384251 12.65699638 12.44740615 12.35898532 12.31006844\n",
            " 12.23070874 12.10738003 11.92571895 11.78156816 11.74401998 11.73230719\n",
            " 11.65186811 11.617223   11.57145576 11.45502649 11.41405651 11.32332326\n",
            " 11.23551682 11.18192443 11.09495607 11.01873038 10.93010826 10.91263119\n",
            " 10.82853232 10.80447802 10.77954476 10.69971101 10.65836437 10.60963091\n",
            " 10.57333075 10.51948702 10.47533499 10.45262003 10.43768111 10.34760553\n",
            " 10.28858725 10.23695178 10.23016134 10.1959859  10.15947262 10.1334389\n",
            " 10.08923168 10.0045741   9.94901613  9.93147351]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7UxQ_SSdOZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "39ca9832-19d1-4551-958c-c4fdb5e42655"
      },
      "source": [
        "print(reduced_features)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.93442333e-02  1.01881367e-01  2.75295137e-01 ...  8.59447196e-04\n",
            "  -4.05432491e-03  1.17735883e-03]\n",
            " [ 1.22762868e+00 -5.75667147e-02  4.53293139e-02 ... -4.58215680e-02\n",
            "   2.28733265e-01 -1.39480246e-01]\n",
            " [ 8.52595440e-02  1.09923636e-01  2.99163741e-01 ... -4.33367911e-03\n",
            "  -1.68230248e-02  1.63597215e-03]\n",
            " ...\n",
            " [ 4.96430409e-04  1.41098458e-03  2.81446000e-03 ... -6.41585508e-03\n",
            "   1.39028755e-02  2.01998477e-02]\n",
            " [ 9.65193781e-01 -2.14868786e-01 -1.12641455e-01 ...  3.01775653e-02\n",
            "  -4.95447618e-02 -4.53177044e-02]\n",
            " [ 7.08646222e-03  2.48330429e-03  7.73408886e-03 ... -1.15429708e-01\n",
            "  -1.25753963e-01  3.36705470e-02]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Glorc3xgOwy",
        "colab_type": "text"
      },
      "source": [
        "## Dense features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9T0sIOWgrsd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "c900a015-480c-4c93-f24a-10e0fda7ffdd"
      },
      "source": [
        "founder_features = pd.read_csv('/content/gdrive/My Drive/vc_modeling/feature_extraction/founder_features/organization_founders_features.csv')\n",
        "founder_features['hash'] = founder_features['org_uuid'].apply(cityhash.CityHash64)\n",
        "founder_features = founder_features.set_index(['hash'], drop=True).drop(['org_uuid'], axis=1)\n",
        "\n",
        "founder_features_only = founder_features.dropna()\n",
        "\n",
        "print(founder_features.index)\n",
        "print(np.count_nonzero(founder_features.index.isin(sparse_join.index)))\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UInt64Index([ 2705467411384211821, 13360469805707984821,  5744847760615345245,\n",
            "             13990853631299335829, 13073125021883633741, 17482404514494389050,\n",
            "              5766560289832673038,  8860273864704446424, 13093060529635406942,\n",
            "              3545623260843038609,\n",
            "             ...\n",
            "             15362439443771027468,  7042924928552646182,  4495539880758712748,\n",
            "               800095033165514664, 17142127829573402143, 11045203277162125921,\n",
            "             17622773241843276753,  9745467974249593237, 11854798340435595808,\n",
            "             12723450708549610702],\n",
            "            dtype='uint64', name='hash', length=198449)\n",
            "165327\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfhmbxNHn9hX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "a321c4b2-b0fd-432d-c2d2-d1e6e3d8dfba"
      },
      "source": [
        "sparse_data = pd.DataFrame(data=reduced_features, index=sparse_join.index, columns=range(np.shape(reduced_features)[1]))\n",
        "print(sparse_data)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                            0         1   ...        98        99\n",
            "0                                         ...                    \n",
            "13685534557686295101  0.019344  0.101881  ... -0.004054  0.001177\n",
            "764015621929367586    1.227629 -0.057567  ...  0.228733 -0.139480\n",
            "10846552445983457719  0.085260  0.109924  ... -0.016823  0.001636\n",
            "5087506707876194815   1.041919 -0.202393  ... -0.019382  0.009674\n",
            "9094535307341385563   1.041080 -0.160387  ...  0.054871  0.070294\n",
            "...                        ...       ...  ...       ...       ...\n",
            "16892862651199510638  0.013808  0.030725  ... -0.215998  0.033216\n",
            "7229716827056183679   0.006609  0.029821  ... -0.001496  0.005391\n",
            "4626564123199390189   0.000496  0.001411  ...  0.013903  0.020200\n",
            "2978566027619600648   0.965194 -0.214869  ... -0.049545 -0.045318\n",
            "9744251496066876000   0.007086  0.002483  ... -0.125754  0.033671\n",
            "\n",
            "[808944 rows x 100 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBWRh2O7jHIZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "e4aafdb2-4777-4a0e-c4e7-01f5ea9ae7f3"
      },
      "source": [
        "all_features = sparse_data.join(founder_features, lsuffix='sparse', rsuffix='dense').fillna(0)\n",
        "all_features.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>founders_top_rank</th>\n",
              "      <th>founders_top_college</th>\n",
              "      <th>founders_max_degree_type_ordinal</th>\n",
              "      <th>founders_max_degree_count</th>\n",
              "      <th>founders_max_founded_other_org</th>\n",
              "      <th>founders_count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13685534557686295101</th>\n",
              "      <td>0.019344</td>\n",
              "      <td>0.101881</td>\n",
              "      <td>0.275295</td>\n",
              "      <td>0.770317</td>\n",
              "      <td>-0.392017</td>\n",
              "      <td>-0.308171</td>\n",
              "      <td>-0.074005</td>\n",
              "      <td>-0.098280</td>\n",
              "      <td>-0.030070</td>\n",
              "      <td>-0.144071</td>\n",
              "      <td>-0.008071</td>\n",
              "      <td>0.014282</td>\n",
              "      <td>-0.037295</td>\n",
              "      <td>-0.020789</td>\n",
              "      <td>0.036092</td>\n",
              "      <td>-0.071545</td>\n",
              "      <td>-0.086070</td>\n",
              "      <td>-0.031744</td>\n",
              "      <td>0.064637</td>\n",
              "      <td>0.036955</td>\n",
              "      <td>-0.036448</td>\n",
              "      <td>0.007317</td>\n",
              "      <td>0.003496</td>\n",
              "      <td>-0.015807</td>\n",
              "      <td>0.031912</td>\n",
              "      <td>-0.018393</td>\n",
              "      <td>-0.005094</td>\n",
              "      <td>-0.011109</td>\n",
              "      <td>-0.011276</td>\n",
              "      <td>-0.008199</td>\n",
              "      <td>0.023240</td>\n",
              "      <td>0.009102</td>\n",
              "      <td>-0.018469</td>\n",
              "      <td>-0.016408</td>\n",
              "      <td>-0.002777</td>\n",
              "      <td>0.001470</td>\n",
              "      <td>-0.022306</td>\n",
              "      <td>0.001711</td>\n",
              "      <td>-0.005529</td>\n",
              "      <td>-0.001948</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.006966</td>\n",
              "      <td>0.005197</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>0.003348</td>\n",
              "      <td>-0.007175</td>\n",
              "      <td>-0.006484</td>\n",
              "      <td>0.003138</td>\n",
              "      <td>0.006962</td>\n",
              "      <td>-0.010271</td>\n",
              "      <td>0.003618</td>\n",
              "      <td>0.001228</td>\n",
              "      <td>0.001735</td>\n",
              "      <td>0.004345</td>\n",
              "      <td>-0.005511</td>\n",
              "      <td>-0.008217</td>\n",
              "      <td>-0.007242</td>\n",
              "      <td>-0.005369</td>\n",
              "      <td>0.006239</td>\n",
              "      <td>0.002315</td>\n",
              "      <td>-0.005934</td>\n",
              "      <td>-0.004468</td>\n",
              "      <td>-0.000590</td>\n",
              "      <td>0.001548</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>-0.002610</td>\n",
              "      <td>0.001855</td>\n",
              "      <td>0.014508</td>\n",
              "      <td>-0.004643</td>\n",
              "      <td>-0.011534</td>\n",
              "      <td>-0.004141</td>\n",
              "      <td>0.002195</td>\n",
              "      <td>0.000859</td>\n",
              "      <td>-0.004054</td>\n",
              "      <td>0.001177</td>\n",
              "      <td>20738.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764015621929367586</th>\n",
              "      <td>1.227629</td>\n",
              "      <td>-0.057567</td>\n",
              "      <td>0.045329</td>\n",
              "      <td>0.307684</td>\n",
              "      <td>-0.378097</td>\n",
              "      <td>1.342761</td>\n",
              "      <td>-0.475410</td>\n",
              "      <td>-0.082669</td>\n",
              "      <td>0.789706</td>\n",
              "      <td>0.067754</td>\n",
              "      <td>-0.074613</td>\n",
              "      <td>-0.046426</td>\n",
              "      <td>0.686327</td>\n",
              "      <td>-0.644056</td>\n",
              "      <td>0.845123</td>\n",
              "      <td>0.151952</td>\n",
              "      <td>0.006688</td>\n",
              "      <td>0.120635</td>\n",
              "      <td>-0.166380</td>\n",
              "      <td>0.068958</td>\n",
              "      <td>-0.078170</td>\n",
              "      <td>-0.011791</td>\n",
              "      <td>0.130785</td>\n",
              "      <td>0.080263</td>\n",
              "      <td>-0.067059</td>\n",
              "      <td>0.004475</td>\n",
              "      <td>0.004838</td>\n",
              "      <td>0.048233</td>\n",
              "      <td>-0.042637</td>\n",
              "      <td>-0.096127</td>\n",
              "      <td>-0.144642</td>\n",
              "      <td>0.276433</td>\n",
              "      <td>0.397258</td>\n",
              "      <td>-0.492175</td>\n",
              "      <td>0.957970</td>\n",
              "      <td>0.143199</td>\n",
              "      <td>0.104934</td>\n",
              "      <td>-0.237836</td>\n",
              "      <td>0.468006</td>\n",
              "      <td>0.037817</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.060790</td>\n",
              "      <td>0.024611</td>\n",
              "      <td>-0.192208</td>\n",
              "      <td>0.032803</td>\n",
              "      <td>-0.115683</td>\n",
              "      <td>0.071774</td>\n",
              "      <td>0.012529</td>\n",
              "      <td>0.049272</td>\n",
              "      <td>0.017279</td>\n",
              "      <td>-0.026792</td>\n",
              "      <td>0.063296</td>\n",
              "      <td>-0.039074</td>\n",
              "      <td>0.065978</td>\n",
              "      <td>0.003437</td>\n",
              "      <td>0.000671</td>\n",
              "      <td>-0.051815</td>\n",
              "      <td>0.037982</td>\n",
              "      <td>0.140371</td>\n",
              "      <td>-0.049215</td>\n",
              "      <td>-0.023172</td>\n",
              "      <td>0.061289</td>\n",
              "      <td>-0.005625</td>\n",
              "      <td>0.028237</td>\n",
              "      <td>0.085048</td>\n",
              "      <td>-0.084880</td>\n",
              "      <td>-0.021828</td>\n",
              "      <td>0.128904</td>\n",
              "      <td>-0.070762</td>\n",
              "      <td>-0.002640</td>\n",
              "      <td>-0.247386</td>\n",
              "      <td>-0.087691</td>\n",
              "      <td>-0.045822</td>\n",
              "      <td>0.228733</td>\n",
              "      <td>-0.139480</td>\n",
              "      <td>122.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10846552445983457719</th>\n",
              "      <td>0.085260</td>\n",
              "      <td>0.109924</td>\n",
              "      <td>0.299164</td>\n",
              "      <td>0.861290</td>\n",
              "      <td>-0.430507</td>\n",
              "      <td>-0.285675</td>\n",
              "      <td>-0.125108</td>\n",
              "      <td>-0.085212</td>\n",
              "      <td>-0.052841</td>\n",
              "      <td>0.301601</td>\n",
              "      <td>0.113324</td>\n",
              "      <td>0.230529</td>\n",
              "      <td>0.696884</td>\n",
              "      <td>-0.310273</td>\n",
              "      <td>0.093155</td>\n",
              "      <td>-0.124344</td>\n",
              "      <td>-0.026946</td>\n",
              "      <td>0.032483</td>\n",
              "      <td>-0.001762</td>\n",
              "      <td>0.030034</td>\n",
              "      <td>-0.029834</td>\n",
              "      <td>-0.043316</td>\n",
              "      <td>-0.029376</td>\n",
              "      <td>-0.015402</td>\n",
              "      <td>-0.018304</td>\n",
              "      <td>-0.009062</td>\n",
              "      <td>-0.062101</td>\n",
              "      <td>-0.009256</td>\n",
              "      <td>-0.007697</td>\n",
              "      <td>-0.003740</td>\n",
              "      <td>-0.002182</td>\n",
              "      <td>-0.015565</td>\n",
              "      <td>-0.001157</td>\n",
              "      <td>-0.017888</td>\n",
              "      <td>0.003070</td>\n",
              "      <td>-0.019593</td>\n",
              "      <td>-0.045027</td>\n",
              "      <td>-0.029899</td>\n",
              "      <td>-0.008037</td>\n",
              "      <td>-0.001272</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014909</td>\n",
              "      <td>0.021263</td>\n",
              "      <td>-0.023598</td>\n",
              "      <td>0.009107</td>\n",
              "      <td>-0.033579</td>\n",
              "      <td>0.013840</td>\n",
              "      <td>0.006350</td>\n",
              "      <td>0.043600</td>\n",
              "      <td>-0.041662</td>\n",
              "      <td>0.012428</td>\n",
              "      <td>0.013264</td>\n",
              "      <td>-0.055690</td>\n",
              "      <td>-0.008123</td>\n",
              "      <td>-0.004781</td>\n",
              "      <td>0.008703</td>\n",
              "      <td>-0.024012</td>\n",
              "      <td>-0.029231</td>\n",
              "      <td>0.032106</td>\n",
              "      <td>0.003477</td>\n",
              "      <td>-0.028051</td>\n",
              "      <td>0.022581</td>\n",
              "      <td>0.005066</td>\n",
              "      <td>0.004184</td>\n",
              "      <td>0.006659</td>\n",
              "      <td>-0.023893</td>\n",
              "      <td>-0.012475</td>\n",
              "      <td>0.029539</td>\n",
              "      <td>-0.008385</td>\n",
              "      <td>-0.011695</td>\n",
              "      <td>-0.001828</td>\n",
              "      <td>0.031314</td>\n",
              "      <td>-0.004334</td>\n",
              "      <td>-0.016823</td>\n",
              "      <td>0.001636</td>\n",
              "      <td>1735.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5087506707876194815</th>\n",
              "      <td>1.041919</td>\n",
              "      <td>-0.202393</td>\n",
              "      <td>-0.075857</td>\n",
              "      <td>0.109147</td>\n",
              "      <td>0.023643</td>\n",
              "      <td>-0.070532</td>\n",
              "      <td>-0.040474</td>\n",
              "      <td>-0.010898</td>\n",
              "      <td>-0.029603</td>\n",
              "      <td>0.387249</td>\n",
              "      <td>0.120527</td>\n",
              "      <td>0.245035</td>\n",
              "      <td>0.727058</td>\n",
              "      <td>-0.284426</td>\n",
              "      <td>0.063152</td>\n",
              "      <td>-0.060805</td>\n",
              "      <td>0.008412</td>\n",
              "      <td>0.042996</td>\n",
              "      <td>-0.034869</td>\n",
              "      <td>0.028239</td>\n",
              "      <td>-0.026684</td>\n",
              "      <td>-0.047513</td>\n",
              "      <td>-0.027730</td>\n",
              "      <td>-0.024480</td>\n",
              "      <td>-0.071931</td>\n",
              "      <td>-0.020039</td>\n",
              "      <td>-0.105923</td>\n",
              "      <td>-0.000254</td>\n",
              "      <td>0.024882</td>\n",
              "      <td>0.021468</td>\n",
              "      <td>-0.005955</td>\n",
              "      <td>-0.043904</td>\n",
              "      <td>0.056714</td>\n",
              "      <td>-0.095702</td>\n",
              "      <td>-0.036477</td>\n",
              "      <td>0.125357</td>\n",
              "      <td>0.432350</td>\n",
              "      <td>0.663625</td>\n",
              "      <td>-0.037359</td>\n",
              "      <td>-0.277945</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.078399</td>\n",
              "      <td>0.097107</td>\n",
              "      <td>0.041287</td>\n",
              "      <td>0.014654</td>\n",
              "      <td>-0.063546</td>\n",
              "      <td>-0.011747</td>\n",
              "      <td>0.008767</td>\n",
              "      <td>0.047182</td>\n",
              "      <td>-0.041398</td>\n",
              "      <td>-0.000322</td>\n",
              "      <td>0.019262</td>\n",
              "      <td>-0.057947</td>\n",
              "      <td>-0.013459</td>\n",
              "      <td>-0.009087</td>\n",
              "      <td>0.016498</td>\n",
              "      <td>-0.017020</td>\n",
              "      <td>-0.037600</td>\n",
              "      <td>0.020123</td>\n",
              "      <td>0.019066</td>\n",
              "      <td>-0.009101</td>\n",
              "      <td>0.014595</td>\n",
              "      <td>0.005995</td>\n",
              "      <td>0.007386</td>\n",
              "      <td>0.011111</td>\n",
              "      <td>-0.037189</td>\n",
              "      <td>-0.020381</td>\n",
              "      <td>0.032332</td>\n",
              "      <td>-0.017438</td>\n",
              "      <td>-0.009020</td>\n",
              "      <td>-0.003751</td>\n",
              "      <td>0.043953</td>\n",
              "      <td>-0.012299</td>\n",
              "      <td>-0.019382</td>\n",
              "      <td>0.009674</td>\n",
              "      <td>59.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9094535307341385563</th>\n",
              "      <td>1.041080</td>\n",
              "      <td>-0.160387</td>\n",
              "      <td>-0.084188</td>\n",
              "      <td>0.061716</td>\n",
              "      <td>-0.073297</td>\n",
              "      <td>0.128333</td>\n",
              "      <td>-0.087326</td>\n",
              "      <td>-0.078266</td>\n",
              "      <td>0.009992</td>\n",
              "      <td>0.070557</td>\n",
              "      <td>0.931831</td>\n",
              "      <td>-0.155165</td>\n",
              "      <td>-0.294538</td>\n",
              "      <td>-0.065990</td>\n",
              "      <td>-0.291644</td>\n",
              "      <td>-0.196866</td>\n",
              "      <td>0.069036</td>\n",
              "      <td>-0.085623</td>\n",
              "      <td>0.043731</td>\n",
              "      <td>-0.161204</td>\n",
              "      <td>0.021061</td>\n",
              "      <td>-0.309758</td>\n",
              "      <td>0.874170</td>\n",
              "      <td>0.387983</td>\n",
              "      <td>-0.360148</td>\n",
              "      <td>-0.023082</td>\n",
              "      <td>-0.078195</td>\n",
              "      <td>-0.031849</td>\n",
              "      <td>0.035606</td>\n",
              "      <td>0.024938</td>\n",
              "      <td>-0.125703</td>\n",
              "      <td>-0.039902</td>\n",
              "      <td>-0.044831</td>\n",
              "      <td>0.076490</td>\n",
              "      <td>0.027981</td>\n",
              "      <td>-0.290259</td>\n",
              "      <td>-0.022896</td>\n",
              "      <td>0.109365</td>\n",
              "      <td>-0.068195</td>\n",
              "      <td>-0.214506</td>\n",
              "      <td>...</td>\n",
              "      <td>0.036618</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>-0.015262</td>\n",
              "      <td>0.048621</td>\n",
              "      <td>0.011390</td>\n",
              "      <td>0.009998</td>\n",
              "      <td>-0.006563</td>\n",
              "      <td>0.012637</td>\n",
              "      <td>-0.003535</td>\n",
              "      <td>-0.046648</td>\n",
              "      <td>-0.018343</td>\n",
              "      <td>0.003269</td>\n",
              "      <td>-0.028208</td>\n",
              "      <td>-0.055698</td>\n",
              "      <td>0.027523</td>\n",
              "      <td>-0.023774</td>\n",
              "      <td>0.016121</td>\n",
              "      <td>0.015818</td>\n",
              "      <td>-0.031063</td>\n",
              "      <td>0.011825</td>\n",
              "      <td>-0.006525</td>\n",
              "      <td>-0.000550</td>\n",
              "      <td>0.035030</td>\n",
              "      <td>0.006653</td>\n",
              "      <td>0.030208</td>\n",
              "      <td>-0.002718</td>\n",
              "      <td>-0.039509</td>\n",
              "      <td>0.036822</td>\n",
              "      <td>0.032012</td>\n",
              "      <td>-0.039576</td>\n",
              "      <td>0.057122</td>\n",
              "      <td>-0.030367</td>\n",
              "      <td>0.054871</td>\n",
              "      <td>0.070294</td>\n",
              "      <td>38172.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 106 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                             0  ...  founders_count\n",
              "0                               ...                \n",
              "13685534557686295101  0.019344  ...             2.0\n",
              "764015621929367586    1.227629  ...             1.0\n",
              "10846552445983457719  0.085260  ...             1.0\n",
              "5087506707876194815   1.041919  ...             9.0\n",
              "9094535307341385563   1.041080  ...             2.0\n",
              "\n",
              "[5 rows x 106 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ_HWAMlSWim",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "d3b97787-cde6-4a4f-fcac-c553a534ac5d"
      },
      "source": [
        "import sklearn.preprocessing\n",
        "\n",
        "features_array = sklearn.preprocessing.normalize(all_features, norm='max', axis=0, copy=False)\n",
        "features = pd.DataFrame(data=features_array, index=all_features.index, columns=range(np.shape(all_features)[1]))\n",
        "print(features)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                           0         1    ...       104       105\n",
            "0                                         ...                    \n",
            "13685534557686295101  0.011911  0.043331  ...  0.012821  0.060606\n",
            "764015621929367586    0.755909 -0.024484  ...  0.012821  0.030303\n",
            "10846552445983457719  0.052498  0.046752  ...  0.000000  0.030303\n",
            "5087506707876194815   0.641559 -0.086080  ...  0.025641  0.272727\n",
            "9094535307341385563   0.641042 -0.068214  ...  0.012821  0.060606\n",
            "...                        ...       ...  ...       ...       ...\n",
            "16892862651199510638  0.008502  0.013068  ...  0.000000  0.000000\n",
            "7229716827056183679   0.004069  0.012683  ...  0.000000  0.060606\n",
            "4626564123199390189   0.000306  0.000600  ...  0.000000  0.000000\n",
            "2978566027619600648   0.594315 -0.091386  ...  0.000000  0.000000\n",
            "9744251496066876000   0.004363  0.001056  ...  0.000000  0.000000\n",
            "\n",
            "[808944 rows x 106 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6InV-GK0e49l",
        "colab_type": "text"
      },
      "source": [
        "# Read Regression Targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzWperFWfA4x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b13176ac-02d2-413c-ec7a-21bd1b7ad1cb"
      },
      "source": [
        "file_names = os.listdir(\"/content/gdrive/My Drive/vc_modeling/regression_targets/\")\n",
        "marks = [int(x.replace('.csv', '')) for x in file_names]\n",
        "print(marks)\n",
        "regression_marks = {}\n",
        "for mark in marks:\n",
        "  rm = pd.read_csv(target_folder + str(mark) + '.csv')[['hash', 'initial_valuation', 'log_valuation_factor']].set_index('hash')\n",
        "  rm = rm[~rm.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
        "  regression_marks[mark] = rm\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[200, 500, 1000, 2000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9hQhkQcY4Jr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "d220766d-cd62-4d80-d46d-3fac49a71fb8"
      },
      "source": [
        "mark_data = regression_marks[200] # pd.read_pickle(\"/content/gdrive/My Drive/vc_modeling/regression_targets/200.pkl\")\n",
        "# mark_data = mark_data[mark_data['log_valuation_factor'] > 0]\n",
        "print(mark_data)\n",
        "print(features)\n",
        "print(np.count_nonzero(mark_data.index.isin(features.index)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                      initial_valuation  log_valuation_factor\n",
            "hash                                                         \n",
            "2053339725337568679         413036820.0              0.000000\n",
            "13360469805707984821          3000000.0              0.266595\n",
            "12201126308526847683         47500000.0              0.000000\n",
            "17482404514494389050          2157880.0              0.000000\n",
            "16923506324318240851          7500000.0              0.000000\n",
            "...                                 ...                   ...\n",
            "4057326795460754576            552105.0              0.000000\n",
            "17393885764651115266         20000000.0              1.011831\n",
            "14785394360939257924        125000000.0              0.000000\n",
            "15207057269115911424         31150000.0              0.000000\n",
            "12723450708549610702            75000.0              1.688395\n",
            "\n",
            "[144569 rows x 2 columns]\n",
            "                           0         1    ...       104       105\n",
            "0                                         ...                    \n",
            "13685534557686295101  0.011911  0.043331  ...  0.012821  0.060606\n",
            "764015621929367586    0.755909 -0.024484  ...  0.012821  0.030303\n",
            "10846552445983457719  0.052498  0.046752  ...  0.000000  0.030303\n",
            "5087506707876194815   0.641559 -0.086080  ...  0.025641  0.272727\n",
            "9094535307341385563   0.641042 -0.068214  ...  0.012821  0.060606\n",
            "...                        ...       ...  ...       ...       ...\n",
            "16892862651199510638  0.008502  0.013068  ...  0.000000  0.000000\n",
            "7229716827056183679   0.004069  0.012683  ...  0.000000  0.060606\n",
            "4626564123199390189   0.000306  0.000600  ...  0.000000  0.000000\n",
            "2978566027619600648   0.594315 -0.091386  ...  0.000000  0.000000\n",
            "9744251496066876000   0.004363  0.001056  ...  0.000000  0.000000\n",
            "\n",
            "[808944 rows x 106 columns]\n",
            "138159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnfflhAPyChF",
        "colab_type": "text"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57NUdx-Xx-9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train Models\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection\n",
        "\n",
        "def regression_analysis(model, train_data, train_values, test_data, test_values):\n",
        "    predicted_train_values = model.predict(train_data)\n",
        "    predicted_test_values = model.predict(test_data)\n",
        "\n",
        "    print(\"Sample values: \", predicted_test_values[:5], test_values[:5])\n",
        "\n",
        "    train_mse = sklearn.metrics.mean_squared_error(train_values, predicted_train_values)\n",
        "    test_mse = sklearn.metrics.mean_squared_error(test_values, predicted_test_values)\n",
        "    train_explained_variance = sklearn.metrics.explained_variance_score(train_values, predicted_train_values)\n",
        "    test_explained_variance = sklearn.metrics.explained_variance_score(test_values, predicted_test_values)\n",
        "\n",
        "    print(\"Train MSE: \", train_mse)\n",
        "    print(\"Train Explained Variance Score: \", train_explained_variance)\n",
        "    print(\"Test MSE: \", test_mse)\n",
        "    print(\"Test Explained Variance Score: \", test_explained_variance)\n",
        "\n",
        "    return model\n",
        "\n",
        "def classification_analysis(model, train_data, train_values, test_data, test_values):\n",
        "    train_values_predicted = model.predict(train_data)\n",
        "    threshold = np.average(train_values_predicted)\n",
        "    train_prediction = train_values_predicted > threshold\n",
        "    train_prediction = train_prediction.astype(np.int32)\n",
        "\n",
        "    test_values_predicted = model.predict(test_data)\n",
        "    test_prediction = test_values_predicted > threshold\n",
        "    test_prediction = test_prediction.astype(np.int32)\n",
        "\n",
        "    train_labels = train_values > threshold\n",
        "    train_labels = train_labels.astype(np.int32)\n",
        "\n",
        "    test_labels = test_values > threshold\n",
        "    test_labels = test_labels.astype(np.int32)\n",
        "\n",
        "    confusion_matrix_large = pd.DataFrame(sklearn.metrics.confusion_matrix(test_labels, test_prediction, labels=[1, 0]),\n",
        "                                    columns=['positive', 'negative'], index=['Truth is +', 'Truth is -'])\n",
        "    print(\"Confusion:\\n\", confusion_matrix_large)\n",
        "    test_acc = sum(test_labels==test_prediction)/len(test_labels)\n",
        "    print(\"Test accuracy: \", test_acc)\n",
        "    train_acc = sum(train_labels==train_prediction)/len(train_labels)\n",
        "    print(\"Train accuracy: \", train_acc)\n",
        "\n",
        "\n",
        "    # # Use the metrics.roc_curve function to get the true positive rate (tpr) and false positive rate (fpr)\n",
        "    # fpr, tpr, thresholds = sklearn.metrics.roc_curve(test_labels, test_probabilities)\n",
        "\n",
        "    # # Get the area under the curve (AUC)\n",
        "    # auc = np.mean(cross_val_score(model, test_data, test_labels, scoring=\"roc_auc\", cv=5))\n",
        "    # print(\"AUC = \" , str(round(auc, 2)))\n",
        "\n",
        "    # # Plot the ROC curve\n",
        "\n",
        "    # plt.xlabel(\"False positive rate (fpr)\")\n",
        "    # plt.ylabel(\"True positive rate (tpr)\")\n",
        "    # plt.plot(fpr, tpr, label='model')\n",
        "    # plt.plot([0, 1], [0, 1], color='k', label=\"random\")\n",
        "    # plt.legend(loc='best')\n",
        "\n",
        "    # plt.figure()\n",
        "    # plt.xlabel(\"Recall\")\n",
        "    # plt.ylabel(\"Precision\")\n",
        "    # precision, recall, _ = sklearn.metrics.precision_recall_curve(test_labels, test_probabilities)\n",
        "    # plt.plot(recall, precision)\n",
        "\n",
        "def train_model_over_mark(model, input_data, mark, filter_unknown=True, hyperparams=None):\n",
        "  print(\"\\nRESULTS FOR\", str(mark), \"DAY MARK:\\n\")\n",
        "  # print(features)\n",
        "  mark_data = regression_marks[mark].dropna()\n",
        "  # print(mark_data)\n",
        "  if filter_unknown:\n",
        "    mark_data = mark_data[mark_data['log_valuation_factor'] != 0]\n",
        "  # Select the data that we have regression targets for\n",
        "\n",
        "  data = input_data[input_data.index.isin(mark_data.index)].sort_index()\n",
        "  # print(data)\n",
        "\n",
        "\n",
        "  # Select the column with log_valuation_factor.\n",
        "  values = mark_data[mark_data.index.isin(input_data.index)]['log_valuation_factor'].sort_index()\n",
        "  # print(values)\n",
        "\n",
        "  train_data, test_data, train_values, test_values = sklearn.model_selection.train_test_split(data, values, test_size=0.25)\n",
        "  if hyperparams:\n",
        "    print(\"Conducting Grid Search\")\n",
        "    search = sklearn.model_selection.GridSearchCV(model, hyperparams)\n",
        "    search.fit(train_data, train_values)\n",
        "    model = search.best_estimator_\n",
        "  \n",
        "  print(\"Trained on\", str(np.shape(train_data)[0]), \"rows.\")\n",
        "  model.fit(train_data, train_values)\n",
        "  regression_analysis(model, train_data, train_values, test_data, test_values)\n",
        "  classification_analysis(model, train_data, train_values, test_data, test_values)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qdukp1WNVzc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "outputId": "9f4f127d-8bb1-49be-857a-3bfcd34d6cc1"
      },
      "source": [
        "print(founder_features_only[:5])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                      founders_top_rank  ...  founders_count\n",
            "hash                                     ...                \n",
            "2705467411384211821            265041.0  ...               4\n",
            "13360469805707984821             6551.0  ...               2\n",
            "13990853631299335829           147007.0  ...               1\n",
            "17482404514494389050           785716.0  ...               1\n",
            "15337030219814864514             1672.0  ...               1\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol1UlbPc-Njh",
        "colab_type": "text"
      },
      "source": [
        "## Lasso"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4zrYwJg-M0E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "adf7fe48-1d10-4fb7-8b09-370ab3baadcb"
      },
      "source": [
        "import sklearn.linear_model\n",
        "\n",
        "for mark in marks:\n",
        "  lasso_model = sklearn.linear_model.LassoCV()\n",
        "  train_model_over_mark(lasso_model, founder_features_only, mark, filter_unknown=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "RESULTS FOR 200 DAY MARK:\n",
            "\n",
            "Trained on 15194 rows.\n",
            "Sample values:  [0.49915806 0.31551135 0.49741146 0.50119549 0.47879544] hash\n",
            "13060534818121295168    0.547134\n",
            "7812462598091645679    -0.284776\n",
            "16246232976548058226    0.414903\n",
            "2058377462233030        0.872386\n",
            "8263846538183928843     2.535350\n",
            "Name: log_valuation_factor, dtype: float64\n",
            "Train MSE:  0.9349399938240103\n",
            "Train Explained Variance Score:  0.006140830689470689\n",
            "Test MSE:  0.9186549200293753\n",
            "Test Explained Variance Score:  0.006851877397404671\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +      1483       586\n",
            "Truth is -      1913      1083\n",
            "Test accuracy:  0.5066140177690029\n",
            "Train accuracy:  0.503751480847703\n",
            "\n",
            "RESULTS FOR 500 DAY MARK:\n",
            "\n",
            "Trained on 15258 rows.\n",
            "Sample values:  [0.87062421 0.58488363 0.87230511 0.85062968 0.92516098] hash\n",
            "16089231626721301270    1.278165\n",
            "12559834587190061872    0.983259\n",
            "15068939385736794845    1.659286\n",
            "18262917739268146752    1.294492\n",
            "2916497670721281501     1.289241\n",
            "Name: log_valuation_factor, dtype: float64\n",
            "Train MSE:  1.697472364035865\n",
            "Train Explained Variance Score:  0.0131367935296558\n",
            "Test MSE:  1.690773619263473\n",
            "Test Explained Variance Score:  0.015370496003695888\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +      1766       629\n",
            "Truth is -      1689      1003\n",
            "Test accuracy:  0.5443286809514448\n",
            "Train accuracy:  0.5406999606763665\n",
            "\n",
            "RESULTS FOR 1000 DAY MARK:\n",
            "\n",
            "Trained on 15289 rows.\n",
            "Sample values:  [1.27001929 1.28658206 1.24884255 1.28481939 1.28235629] hash\n",
            "1594611353111538634     0.951799\n",
            "7849585232376122086     1.304428\n",
            "6414185953083838195     2.609414\n",
            "16475796231248482146    2.605865\n",
            "6047317699674532303     3.438239\n",
            "Name: log_valuation_factor, dtype: float64\n",
            "Train MSE:  2.399987582839923\n",
            "Train Explained Variance Score:  0.022383750650629253\n",
            "Test MSE:  2.34544901228145\n",
            "Test Explained Variance Score:  0.02611958763506994\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +      1799       633\n",
            "Truth is -      1643      1022\n",
            "Test accuracy:  0.5534628212674122\n",
            "Train accuracy:  0.5579174569952253\n",
            "\n",
            "RESULTS FOR 2000 DAY MARK:\n",
            "\n",
            "Trained on 15271 rows.\n",
            "Sample values:  [0.76460095 1.35472514 0.5833569  1.51061223 0.82205915] hash\n",
            "717496476887362778     -0.026668\n",
            "18035680997722364539    2.766741\n",
            "17561085571384407708   -0.575364\n",
            "8245552258794777853     0.803350\n",
            "15384698535118723463   -1.540445\n",
            "Name: log_valuation_factor, dtype: float64\n",
            "Train MSE:  3.0289539627473796\n",
            "Train Explained Variance Score:  0.032786156378763365\n",
            "Test MSE:  2.9569049602872037\n",
            "Test Explained Variance Score:  0.02898691341752735\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +      1810       590\n",
            "Truth is -      1629      1062\n",
            "Test accuracy:  0.5641327833431545\n",
            "Train accuracy:  0.5581166917687119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ksr5r4cuc0",
        "colab_type": "text"
      },
      "source": [
        "## Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5PRMVseeLBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f12accc-6bd9-40e1-b1ac-f3348634a3be"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "models = []\n",
        "hyperdict = {'n_estimators': [20, 40, 60, 80, 100], 'min_samples_split':[10, 20, 30]}\n",
        "\n",
        "for mark in marks:\n",
        "  tree_model = RandomForestRegressor(criterion='mse', max_depth=4)\n",
        "  train_model_over_mark(tree_model, features, mark, filter_unknown=True)\n",
        "  models.append(tree_model)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "RESULTS FOR 200 DAY MARK:\n",
            "\n",
            "Trained on 39445 rows.\n",
            "Sample values:  [0.29468278 0.26968576 0.27967579 0.34657118 0.26414289] hash\n",
            "17196882803872297573    0.793497\n",
            "17437293347975446381    0.391368\n",
            "12524796904998413897    0.603349\n",
            "9915292093174236763     0.377074\n",
            "6806748359671183443     1.061899\n",
            "Name: log_valuation_factor, dtype: float64\n",
            "Train MSE:  0.8365104172391014\n",
            "Train Explained Variance Score:  0.0264739351958716\n",
            "Test MSE:  0.886216702116822\n",
            "Test Explained Variance Score:  0.014646187393217325\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +      2342      3275\n",
            "Truth is -      2326      5206\n",
            "Test accuracy:  0.5740360483686973\n",
            "Train accuracy:  0.5833692483204462\n",
            "\n",
            "RESULTS FOR 500 DAY MARK:\n",
            "\n",
            "Trained on 39573 rows.\n",
            "Sample values:  [0.54162186 0.4728713  0.47605769 0.50817659 1.24851547] hash\n",
            "17986535554932535408    0.369003\n",
            "13682655561331062899    0.034386\n",
            "7220143968930273359     1.203973\n",
            "1814416024560022572     0.405465\n",
            "3907786077573467582     0.556100\n",
            "Name: log_valuation_factor, dtype: float64\n",
            "Train MSE:  1.5989304094915102\n",
            "Train Explained Variance Score:  0.0397950049151744\n",
            "Test MSE:  1.597119789020767\n",
            "Test Explained Variance Score:  0.025482623728533493\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +      2305      4001\n",
            "Truth is -      1627      5259\n",
            "Test accuracy:  0.5733778047301394\n",
            "Train accuracy:  0.5832512066307837\n",
            "\n",
            "RESULTS FOR 1000 DAY MARK:\n",
            "\n",
            "Trained on 39627 rows.\n",
            "Sample values:  [0.64726294 0.58515275 0.73816923 1.30257251 0.86542251] hash\n",
            "4912302049938250921     0.045542\n",
            "12767215883700926324    0.721143\n",
            "4815705222754493840     1.211534\n",
            "2904079405828177849     1.873024\n",
            "13939136139250326153   -0.606644\n",
            "Name: log_valuation_factor, dtype: float64\n",
            "Train MSE:  2.20028029828506\n",
            "Train Explained Variance Score:  0.056781882778318415\n",
            "Test MSE:  2.1693275685489444\n",
            "Test Explained Variance Score:  0.04339828048118899\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +      2388      4097\n",
            "Truth is -      1529      5195\n",
            "Test accuracy:  0.5740782799606329\n",
            "Train accuracy:  0.5873520579402932\n",
            "\n",
            "RESULTS FOR 2000 DAY MARK:\n",
            "\n",
            "Trained on 39583 rows.\n",
            "Sample values:  [0.8405467  0.5622622  0.83181932 1.27718486 0.85405945] hash\n",
            "13145018879600407740    2.397584\n",
            "3631840594001094829    -0.095310\n",
            "6560356209508231211    -0.052703\n",
            "5278570755035095359     3.036554\n",
            "3551965681733738242     4.887537\n",
            "Name: log_valuation_factor, dtype: float64\n",
            "Train MSE:  2.675784066539136\n",
            "Train Explained Variance Score:  0.06837725082449653\n",
            "Test MSE:  2.71764695874655\n",
            "Test Explained Variance Score:  0.06421922892945064\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +      2678      3722\n",
            "Truth is -      1760      5035\n",
            "Test accuracy:  0.5845395983327017\n",
            "Train accuracy:  0.5909355026147589\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nyz3a63EcWxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk-T0iO2xtaQ",
        "colab_type": "text"
      },
      "source": [
        "## SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_GVqKW6xxhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.linear_model\n",
        "\n",
        "sgd = sklearn.linear_model.SGDRegressor()\n",
        "\n",
        "for mark in marks:\n",
        "  train_model_over_mark(sgd, features, mark, filter_unknown=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmcPRYYmzrp-",
        "colab_type": "text"
      },
      "source": [
        "# SVR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE03MWnSztYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "fc6a1493-3056-4dd7-f171-53e010fafd04"
      },
      "source": [
        "import sklearn.svm\n",
        "# Warning: this takes a really long time.\n",
        "\n",
        "for mark in marks:\n",
        "  svm = sklearn.svm.SVR(C=0.001)\n",
        "  train_model_over_mark(svm, features, mark, filter_unknown=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "RESULTS FOR 200 DAY MARK:\n",
            "\n",
            "Trained on 39445 rows.\n",
            "Sample values:  [0.28225426 0.2599009  0.24700179 0.28501875 0.24145089] hash\n",
            "9922409125951720504    0.142623\n",
            "3723860927443974661    2.646436\n",
            "944842188742108698     1.171183\n",
            "9874696254773956310    0.020181\n",
            "2913120074975965592   -0.017817\n",
            "Name: log_valuation_factor, dtype: float64\n",
            "Train MSE:  0.867838754693294\n",
            "Train Explained Variance Score:  0.00768389167151462\n",
            "Test MSE:  0.8890430168748725\n",
            "Test Explained Variance Score:  0.005207680788291258\n",
            "Confusion:\n",
            "             positive  negative\n",
            "Truth is +      3604      2924\n",
            "Truth is -      3014      3607\n",
            "Test accuracy:  0.5484067229447106\n",
            "Train accuracy:  0.5563696285967803\n",
            "\n",
            "RESULTS FOR 500 DAY MARK:\n",
            "\n",
            "Trained on 39573 rows.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLN28KUEnDVG",
        "colab_type": "text"
      },
      "source": [
        "## MultiLayer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53e0q3p3nHWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn.neural_network\n",
        "\n",
        "models = []\n",
        "for mark in marks:\n",
        "  mlp = sklearn.neural_network.MLPRegressor(alpha=1e-2, hidden_layer_sizes=(100, 50, 20), max_iter=500)\n",
        "  train_model_over_mark(mlp, features, mark, filter_unknown=True)\n",
        "  models.append(mlp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3t6C4128MMq",
        "colab_type": "text"
      },
      "source": [
        "## Save Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzw2iKOr6FL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the models\n",
        "import pickle\n",
        "print(models)\n",
        "for mark, model in zip(marks, models):\n",
        "  print(model)\n",
        "  pickle.dump(model, open('/content/gdrive/My Drive/vc_modeling/models/' + str(mark) + '.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UivkBRqlA4UM",
        "colab_type": "text"
      },
      "source": [
        "# Export Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVg9x0AH6qoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the models\n",
        "import pickle\n",
        "models = []\n",
        "for mark in marks:\n",
        "\n",
        "  model = pickle.load(open('/content/gdrive/My Drive/vc_modeling/models/' + str(int(mark)) + '.pkl', 'rb'))\n",
        "  models.append(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK73IoyvvQPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "org_info = pd.read_csv(\"/content/gdrive/My Drive/vc_modeling/data/crunchbase_bulk_export/organizations.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65FZRRvezF6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "org_info['hash'] = org_info['uuid'].apply(cityhash.CityHash64)\n",
        "org_info = org_info.set_index('hash')\n",
        "org_info = org_info[['uuid', 'name', 'created_at']]\n",
        "org_info.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "881FEeGvxx-M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "org_info_join = org_info.copy()\n",
        "\n",
        "print(marks)\n",
        "for model, mark in zip(models, marks):\n",
        "  truth = regression_marks[mark][['initial_valuation', 'log_valuation_factor']].copy()\n",
        "  # print(truth)\n",
        "  prediction_array = model.predict(features)\n",
        "\n",
        "  prediction = pd.DataFrame(data=prediction_array, index=features.index, columns=['prediction_' + str(mark)])\n",
        "  uuid_hash = features.index\n",
        "  pred_truth = truth.join(prediction)\n",
        "  pred_truth = pred_truth.rename(columns={'log_valuation_factor':'truth_' + str(mark)})\n",
        "  org_info_join = org_info_join.join(pred_truth, rsuffix=mark)\n",
        "\n",
        "org_info_join org_info_join.set_index('uuid', drop=True)\n",
        "org_info_join.head()\n",
        "\n",
        "org_info_join.to_csv('/content/gdrive/My Drive/vc_modeling/model_output/predictions.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R0LykdHtIRt",
        "colab_type": "text"
      },
      "source": [
        "# Comparison to Random"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x9kg7OHtHpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def compare_to_random(org_info_with_pred):\n",
        "  clean_org_info = org_info_with_pred.dropna()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}