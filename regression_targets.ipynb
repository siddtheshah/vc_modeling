{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "regression_targets.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP97mWc7ySk5X46/JRJb5mc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddtheshah/vc_modeling/blob/master/regression_targets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-pY4jSsgm_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from copy import deepcopy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfZWKIoJgxTF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "c2905c22-0f1e-456f-e6a8-fbc7e3e5c7ba"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDnCUjqjg4Z7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "8c776239-e171-405b-cfaf-84b6ff1b85b7"
      },
      "source": [
        "file_names = os.listdir(\"/content/gdrive/My Drive/vc_modeling/data/crunchbase_bulk_export/\")\n",
        "df_names = [x[:-4] for x in file_names]\n",
        "print(df_names)\n",
        "\n",
        "dfs = [pd.read_csv(\"/content/gdrive/My Drive/vc_modeling/data/crunchbase_bulk_export/\"+x) for x in file_names]\n",
        "df_dict = dict(zip(df_names, dfs))\n",
        "print(df_dict.keys())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['category_groups', 'funding_rounds', 'people', 'checksum', 'people_descriptions', 'investors', 'organization_descriptions', 'investment_partners', 'event_appearances', 'organizations', 'org_parents', 'jobs', 'acquisitions', 'funds', 'ipos', 'degrees', 'investments', 'events']\n",
            "dict_keys(['category_groups', 'funding_rounds', 'people', 'checksum', 'people_descriptions', 'investors', 'organization_descriptions', 'investment_partners', 'event_appearances', 'organizations', 'org_parents', 'jobs', 'acquisitions', 'funds', 'ipos', 'degrees', 'investments', 'events'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odiyosZig9Wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "funding_data = deepcopy(\n",
        "    df_dict[\"funding_rounds\"][['org_uuid', 'announced_on', 'post_money_valuation_usd']]).rename(\n",
        "    columns={\"announced_on\": \"date\", \"post_money_valuation_usd\": \"valuation\"})\n",
        "acquisitions_data = deepcopy(df_dict[\"acquisitions\"][['acquiree_uuid', 'acquired_on', 'price_usd']]).rename(\n",
        "    columns={\"acquiree_uuid\": \"org_uuid\", \"acquired_on\": \"date\", \"price_usd\": \"valuation\"})\n",
        "ipos_data = deepcopy(df_dict[\"ipos\"][['org_uuid', 'went_public_on', 'valuation_price_usd']]).rename(\n",
        "    columns={\"went_public_on\": \"date\", \"valuation_price_usd\": \"valuation\"})\n",
        "\n",
        "types = df_dict[\"acquisitions\"][['type']].values\n",
        "acquisition_types = df_dict[\"acquisitions\"][['acquisition_type']].values\n",
        "acquisitions_data['event'] = [types[x][0] if not isinstance(acquisition_types[x][0], str) \n",
        "                              else acquisition_types[x][0] for x in range(len(acquisitions_data))]\n",
        "funding_data['event'] = pd.Series([x[0] for x in df_dict[\"funding_rounds\"]['name'].str.split(\" - \")])\n",
        "ipos_data['event'] = df_dict['ipos']['stock_exchange_symbol']\n",
        "\n",
        "timelines = pd.concat([funding_data, acquisitions_data, ipos_data])\n",
        "\n",
        "# Normalize dates\n",
        "\n",
        "timelines.sort_values(by=['org_uuid', 'date'], inplace=True)\n",
        "timelines.reset_index(inplace=True, drop=True)\n",
        "\n",
        "timelines.set_index(['org_uuid'])\n",
        "grouped_timelines = timelines.groupby(['org_uuid'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUxeCM3gkyEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "company_timelines = {}\n",
        "\n",
        "milli_per_day = 86400  # Used for converting Unix time\n",
        "\n",
        "for company_id in timelines['org_uuid']:\n",
        "  timeline = grouped_timelines.get_group(company_id).copy()\n",
        "  timeline = timeline[timeline['valuation'].notnull()]               # We only care about times at which there is a valuation of the company.\n",
        "  timeline.reset_index(inplace=True, drop=True)\n",
        "  if timeline.empty:\n",
        "    continue\n",
        "\n",
        "  # print(timeline['date'])\n",
        "  day_zero = datetime.datetime.strptime(timeline['date'].iloc[0], '%Y-%m-%d').timestamp()\n",
        "  parsed_dates = pd.Series([datetime.datetime.strptime(x, '%Y-%m-%d').timestamp() for x in timeline['date']])\n",
        "  norm_dates = (parsed_dates - day_zero)/milli_per_day\n",
        "  timeline['norm_dates'] = norm_dates\n",
        "  company_timelines[company_id] = timeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvMFwBjokziL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for company_id, timeline in company_timelines.items():\n",
        "  initial_valuation = timeline['valuation'].iloc[0]\n",
        "  timeline['initial_val'] = initial_valuation\n",
        "  norm_valuations = pd.Series([np.log(value/initial_valuation) for value in timeline['valuation']])\n",
        "  print(norm_valuations)\n",
        "  print(timeline)\n",
        "  break\n",
        "  timeline['norm_valuations'] = norm_valuations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMz97LRqhF5Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import interpolate\n",
        "from google.colab import files\n",
        "\n",
        "def get_valuation_interpolations(timelines, day):\n",
        "  company_names = []\n",
        "  regression_targets = []\n",
        "  initial_valuations = []\n",
        "  for company, timeline in timelines.items():\n",
        "    company_names.append(company)\n",
        "    x = timeline['norm_dates'].tolist()\n",
        "    y = timeline['norm_valuations'].tolist()\n",
        "    # print(\"X:\", x)\n",
        "    # print(\"Y:\", y)\n",
        "    if len(x) < 2 or len(y) < 2:\n",
        "      regression_targets.append(0)\n",
        "      continue\n",
        "    \n",
        "    if day > x[-1]:\n",
        "      regression_targets.append(y[-1])  # Don't extrapolate the data at all.\n",
        "      continue\n",
        "\n",
        "    interpolator = interpolate.interp1d(x, y)\n",
        "    regression_target = interpolator(day)\n",
        "    regression_targets.append(regression_target)\n",
        "    initial_valuations.append(timeline['initial_valuation'])\n",
        "\n",
        "  return pd.DataFrame(list(zip(company_names, initial_valuations, regression_targets)), columns=[\"company\", \"initial_valuation\", \"log_valuation_factor\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5DxVI4okw6U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "e5b8a13a-598b-411e-805e-10d41161f773"
      },
      "source": [
        "for day in [200, 500, 1000, 2000]:\n",
        "  frame = get_valuation_interpolations(company_timelines, day)\n",
        "  frame.to_csv(str(day) + '.csv')\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-3cc18a31d57d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mday\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_valuation_interpolations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompany_timelines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'write_csv'"
          ]
        }
      ]
    }
  ]
}