{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "category_features.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNP2UgSk9gNPwrqAP8+nVmg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddtheshah/vc_modeling/blob/master/category_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLj02yHu8Fm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from copy import deepcopy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNCUUReS8P_O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "15eadc9d-1916-418e-a877-7d8b12674d34"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLIkmx-Q8Sm0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "cc9f4e2b-cdca-45da-c653-9b271edbea2c"
      },
      "source": [
        "file_names = os.listdir(\"/content/gdrive/My Drive/vc_modeling/data/crunchbase_bulk_export/\")\n",
        "df_names = [x[:-4] for x in file_names]\n",
        "print(df_names)\n",
        "\n",
        "dfs = [pd.read_csv(\"/content/gdrive/My Drive/vc_modeling/data/crunchbase_bulk_export/\"+x) for x in file_names]\n",
        "df_dict = dict(zip(df_names, dfs))\n",
        "print(df_dict.keys())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['category_groups', 'funding_rounds', 'people', 'checksum', 'people_descriptions', 'investors', 'organization_descriptions', 'investment_partners', 'event_appearances', 'organizations', 'org_parents', 'jobs', 'acquisitions', 'funds', 'ipos', 'degrees', 'investments', 'events']\n",
            "dict_keys(['category_groups', 'funding_rounds', 'people', 'checksum', 'people_descriptions', 'investors', 'organization_descriptions', 'investment_partners', 'event_appearances', 'organizations', 'org_parents', 'jobs', 'acquisitions', 'funds', 'ipos', 'degrees', 'investments', 'events'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTkfMfnE8Xl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "44664013-253b-4c05-f7d8-b49621af65cc"
      },
      "source": [
        "import collections\n",
        "\n",
        "org_info = df_dict['organizations']\n",
        "\n",
        "# Filter down the data purely to investees\n",
        "org_info = org_info[org_info['primary_role'] == 'company']\n",
        "\n",
        "# Select organization data\n",
        "# Clean the df\n",
        "\n",
        "comma_sep_list_vars = ['category_list', 'category_groups_list']\n",
        "categorical_vars = ['region']\n",
        "other_vars = ['uuid']\n",
        "org_info_selected = org_info[other_vars + categorical_vars + comma_sep_list_vars]\n",
        "org_info_selected = org_info_selected.dropna()\n",
        "org_info_selected = org_info_selected.reset_index(drop=True)\n",
        "\n",
        "vocabulary = collections.Counter()\n",
        "\n",
        "iter = 0\n",
        "test_stop = -1\n",
        "total_rows = np.shape(org_info_selected)[0]\n",
        "print(\"Processing \", total_rows, \" rows.\")\n",
        "for (i, row) in org_info_selected.iterrows():\n",
        "  if iter == test_stop:\n",
        "    break\n",
        "  iter += 1\n",
        "  if iter % 100000 == 0:\n",
        "    print(iter, \" of \", total_rows)\n",
        "\n",
        "  for col, _ in row.iteritems():\n",
        "    if col in comma_sep_list_vars:\n",
        "      # Break categories into unigrams\n",
        "      token_lists = [x.split(' ') for x in row[col].strip().split(',')]\n",
        "      tokens = []\n",
        "      for token_list in token_lists:\n",
        "        for token in token_list:\n",
        "          tokens.append(token)\n",
        "\n",
        "      for token in tokens:\n",
        "        vocabulary[token] += 1\n",
        "\n",
        "# Filter low count words\n",
        "low_count_threshold = 15\n",
        "keys_to_remove = []\n",
        "for word, count in vocabulary.items():\n",
        "  if count < low_count_threshold:\n",
        "    keys_to_remove.append(word)\n",
        "\n",
        "for key in keys_to_remove:\n",
        "  del vocabulary[key]\n",
        "\n",
        "# Now rank and filter by document frequency. The top half frequent words will be dropped\n",
        "key_list = list(vocabulary.keys())\n",
        "fraction_to_drop = .25\n",
        "chop = sorted(key_list, key=lambda x: vocabulary[x])[-int(.5*len(key_list)):]\n",
        "for key in chop:\n",
        "  del vocabulary[key]\n",
        "\n",
        "# Create index to word mapping\n",
        "word_to_index = {}\n",
        "word_index = 0\n",
        "for word, count in vocabulary.items():\n",
        "  word_to_index[word] = word_index\n",
        "  word_index += 1\n",
        "\n",
        "print(word_to_index)\n",
        "vocabulary_size = word_index"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing  808944  rows.\n",
            "100000  of  808944\n",
            "200000  of  808944\n",
            "300000  of  808944\n",
            "400000  of  808944\n",
            "500000  of  808944\n",
            "600000  of  808944\n",
            "700000  of  808944\n",
            "800000  of  808944\n",
            "{'SMS': 0, 'EBooks': 1, 'Embedded': 2, 'Hedge': 3, 'Funds': 4, 'Contact': 5, 'Knowledge': 6, 'Currency': 7, 'GPS': 8, 'Bookmarking': 9, 'Browser': 10, 'Extensions': 11, 'Dating': 12, 'Chat': 13, 'Forums': 14, 'Celebrity': 15, 'Radio': 16, 'Facebook': 17, 'World': 18, 'Podcast': 19, 'Stock': 20, 'Exchange': 21, 'Discovery': 22, 'Ediscovery': 23, 'Physical': 24, 'Flash': 25, 'Concerts': 26, 'Nightlife': 27, 'National': 28, 'Freelance': 29, 'to': 30, 'Classifieds': 31, 'Reviews': 32, 'Cooking': 33, 'Recipes': 34, 'Teenagers': 35, 'WebOS': 36, 'Promotion': 37, 'Universities': 38, 'Virtualization': 39, 'on': 40, 'Demand': 41, 'Angel': 42, 'Transaction': 43, 'Outdoors': 44, 'Recreation': 45, 'MMO': 46, 'Tobacco': 47, 'Alumni': 48, 'Reputation': 49, 'Image': 50, 'CMS': 51, 'Grocery': 52, 'IaaS': 53, 'Meeting': 54, 'Independent': 55, 'Conferencing': 56, 'Casual': 57, 'Journalism': 58, 'Card': 59, 'Gambling': 60, 'Freemium': 61, 'Targeting': 62, 'Micro': 63, 'Syndication': 64, 'Contests': 65, 'Reservations': 66, 'Fantasy': 67, 'Speech': 68, 'Psychology': 69, 'Child': 70, 'Elder': 71, 'Tutoring': 72, 'Family': 73, 'Parenting': 74, 'Google': 75, 'Casino': 76, 'Motion': 77, 'Capture': 78, 'ISP': 79, 'Affiliate': 80, 'Charity': 81, 'Humanitarian': 82, 'Presentations': 83, 'DIY': 84, 'Geospatial': 85, 'DSP': 86, 'Application': 87, 'Performance': 88, 'Racing': 89, 'Elderly': 90, 'Cycling': 91, 'Satellite': 92, 'Power': 93, 'Grid': 94, 'Politics': 95, 'Homeland': 96, 'Q&A': 97, 'Fraud': 98, 'Detection': 99, 'Branding': 100, 'Spam': 101, 'Filtering': 102, 'Wedding': 103, 'GovTech': 104, 'DRM': 105, 'Text': 106, 'Cards': 107, 'Debit': 108, 'Young': 109, 'Adults': 110, 'Intelligent': 111, 'Venues': 112, 'Musical': 113, 'Instruments': 114, 'Brewing': 115, 'Price': 116, 'Comparison': 117, 'Religion': 118, 'Collaborative': 119, 'Consumption': 120, 'Fuel': 121, 'Guides': 122, 'Battery': 123, 'Equestrian': 124, 'Accommodations': 125, 'Vacation': 126, 'Dietary': 127, 'Supplements': 128, 'Funding': 129, 'Collectibles': 130, 'Visual': 131, 'Funerals': 132, 'Baby': 133, 'Resorts': 134, 'A/B': 135, 'Testing': 136, 'SNS': 137, 'RFID': 138, 'Biometrics': 139, 'Fleet': 140, 'Domain': 141, 'Registrar': 142, 'Theatre': 143, 'Vertical': 144, 'eSports': 145, 'In-Flight': 146, 'Bureau': 147, 'Record': 148, '(EHR)': 149, 'Skill': 150, 'Assessment': 151, 'Linux': 152, 'Workforce': 153, 'Reading': 154, 'Presentation': 155, 'Interaction': 156, 'CivicTech': 157, 'Server': 158, 'Scheduling': 159, 'Procurement': 160, 'Quantum': 161, 'Neuroscience': 162, 'Flowers': 163, 'E-Signature': 164, 'Eyewear': 165, 'Unified': 166, 'Twitter': 167, 'Retargeting': 168, 'Space': 169, 'Craft': 170, 'Beer': 171, 'Laundry': 172, 'Dry-cleaning': 173, 'Cause': 174, 'Auto': 175, 'Charter': 176, 'Schools': 177, 'QR': 178, 'Codes': 179, 'Audiobooks': 180, 'Usability': 181, 'Optical': 182, 'Operating': 183, 'Diabetes': 184, 'Soccer': 185, 'GPU': 186, 'Retirement': 187, 'Parking': 188, 'Comics': 189, 'Handmade': 190, 'Compliance': 191, 'Desktop': 192, 'Facial': 193, 'Adult': 194, 'LGBT': 195, 'Biofuel': 196, 'CAD': 197, 'Call': 198, 'Recreational': 199, 'Emerging': 200, 'Markets': 201, 'Simulation': 202, 'Adventure': 203, 'College': 204, 'Group': 205, 'Buying': 206, 'NFC': 207, 'Nightclubs': 208, 'Tennis': 209, 'Preparation': 210, 'Garden': 211, 'Label': 212, 'Tour': 213, 'Operator': 214, '(EDA)': 215, 'Ride': 216, 'First': 217, 'Aid': 218, 'Fast-Moving': 219, 'Alternative': 220, 'Medicine': 221, 'Archiving': 222, 'Textbook': 223, 'Wired': 224, 'American': 225, 'Football': 226, 'Bakery': 227, 'Economy': 228, 'Fertility': 229, 'Windows': 230, 'Phone': 231, 'Registry': 232, 'Diving': 233, 'Courier': 234, 'Same': 235, 'Day': 236, 'Tea': 237, 'Console': 238, 'Xbox': 239, 'Facilities': 240, 'Assistive': 241, 'Boating': 242, 'Vending': 243, 'Concessions': 244, 'Field-Programmable': 245, 'Gate': 246, 'Array': 247, '(FPGA)': 248, 'Continuing': 249, 'Primary': 250, 'Secondary': 251, 'STEM': 252, 'Vocational': 253, 'Performing': 254, 'Arts': 255, 'Leasing': 256, 'Assisted': 257, 'Living': 258, 'Sponsorship': 259, 'macOS': 260, 'Green': 261, 'Playstation': 262, 'Assistant': 263, 'Facility': 264, 'Wind': 265, 'Railroad': 266, \"Men's\": 267, 'Biomass': 268, 'Emergency': 269, 'Aquaculture': 270, 'Laser': 271, 'Horticulture': 272, 'Indoor': 273, 'Positioning': 274, 'Museums': 275, 'Historical': 276, 'Sites': 277, 'Car': 278, 'Quality': 279, 'Assurance': 280, 'Cell': 281, 'Golf': 282, 'Fossil': 283, 'Fuels': 284, 'Housekeeping': 285, 'Nutraceutical': 286, 'Animal': 287, 'Feed': 288, 'Janitorial': 289, 'Glass': 290, 'Debt': 291, 'Collections': 292, 'Cities': 293, 'Nuclear': 294, 'Forestry': 295, 'Bioinformatics': 296, 'Intrusion': 297, 'Self-Storage': 298, 'Pollution': 299, 'Control': 300, 'Office': 301, 'Administration': 302, 'Hydroponics': 303, 'Wood': 304, 'Cosmetic': 305, 'Surgery': 306, 'Timeshare': 307, 'Homeless': 308, 'Shelter': 309, 'Underserved': 310, 'Serious': 311, 'Sex': 312, 'Basketball': 313, 'Nursing': 314, 'Landscaping': 315, 'Franchise': 316, 'Lingerie': 317, 'Assistance': 318, 'Livestock': 319, 'Farmers': 320, 'Field': 321, 'Multi-level': 322, 'Hockey': 323, 'Last': 324, 'Mile': 325, 'Outpatient': 326, 'Edutainment': 327, 'MOOC': 328, 'Mall': 329, 'Taxi': 330, 'Specific': 331, 'Integrated': 332, 'Circuit': 333, '(ASIC)': 334, 'Limousine': 335, 'Penetration': 336, 'Catering': 337, 'Broker': 338, 'Trade': 339, 'Shows': 340, 'Trucks': 341, 'Hunting': 342, 'Veterinary': 343, 'Paper': 344, 'Collection': 345, 'Rehabilitation': 346, 'Swimming': 347, 'Fruit': 348, 'Ethereum': 349, 'Quantified': 350, 'Self': 351, 'Parks': 352, 'Made': 353, 'Order': 354, 'Millennials': 355, 'Winery': 356, 'Surfing': 357, 'Distillery': 358, 'Seafood': 359, 'Prediction': 360, 'Drone': 361, 'Cricket': 362, 'Confectionery': 363, 'Skiing': 364, 'Amusement': 365, 'Park': 366, 'Arcade': 367, 'Baseball': 368, 'Sailing': 369, 'Ports': 370, 'Harbors': 371, 'Timber': 372, 'Foundries': 373, 'Corrections': 374}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5YFUsxk8Z3T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "87e4bdfb-a533-4ab7-a26e-3bd55b695c60"
      },
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "import collections\n",
        "\n",
        "# Load our filtered vocabulary if we've saved it on disk.\n",
        "\n",
        "row_indices = []\n",
        "col_indices = []\n",
        "data_values = []\n",
        "\n",
        "iter = 0\n",
        "test_stop = -1\n",
        "total_rows = np.shape(org_info_selected)[0]\n",
        "print(\"Processing \", total_rows, \" rows.\")\n",
        "for (i, row) in org_info_selected.iterrows():\n",
        "  if iter == test_stop:\n",
        "    break\n",
        "  iter += 1\n",
        "  if iter % 100000 == 0:\n",
        "    print(iter, \" of \", total_rows)\n",
        "\n",
        "  for col, _ in row.iteritems():\n",
        "    if col in comma_sep_list_vars:\n",
        "      # Break categories into unigrams\n",
        "      unigram_counts = collections.Counter()\n",
        "      token_lists = [x.split(' ') for x in row[col].strip().split(',')]\n",
        "      for token_list in token_lists:\n",
        "        for token in token_list:\n",
        "          unigram_counts[token] += 1\n",
        "      \n",
        "      for unigram, count in unigram_counts.items():\n",
        "        if unigram in vocabulary:\n",
        "          row_indices.append(i)\n",
        "          col_indices.append(word_to_index[unigram])\n",
        "          data_values.append(count)\n",
        "    elif col in categorical_vars:\n",
        "      value = row[col]\n",
        "      if value in vocabulary:\n",
        "        row_indices.append(i)\n",
        "        col_indices.append(word_to_index[value])\n",
        "        data_values.append(1)\n",
        "\n",
        "unclustered = csr_matrix((data_values, (row_indices, col_indices)))\n",
        "print(unclustered.toarray().shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing  808944  rows.\n",
            "100000  of  808944\n",
            "200000  of  808944\n",
            "300000  of  808944\n",
            "400000  of  808944\n",
            "500000  of  808944\n",
            "600000  of  808944\n",
            "700000  of  808944\n",
            "800000  of  808944\n",
            "(808944, 375)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJMWPc-d8oui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.sparse\n",
        "\n",
        "scipy.sparse.save_npz('/category_features.npz', unclustered)\n",
        "companies = org_info_selected['uuid']\n",
        "companies.to_csv('/companies.csv')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVwC_DgmYWnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}